{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<style>\n",
        "body {\n",
        "    font-family: 'Arial', sans-serif;  /* Change to your desired font */\n",
        "}\n",
        "</style>\n",
        "\n",
        "\n",
        "```\n",
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# Author: pcorreia@google.com\n",
        "```"
      ],
      "metadata": {
        "id": "hpyN8_QD5cIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metadata Enrichment with Gemini âœ¨\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "clc15c3rxBm3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview\n"
      ],
      "metadata": {
        "id": "xmS3VhBW8ub4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Purpose of this notebook**\n",
        "\n",
        "The purpose of this notebook is to showcase how Gemini works with multi-modal inputs to extract information. More info on Gemini's multi-modality [here](https://cloud.google.com/use-cases/multimodal-ai?hl=en).\n",
        "\n",
        "Specifically the scenario we are showcasing in this notebook is the creation of a pipeline that given a set of medical report in a Google Cloud Storage bucket is able to create metadata that represents all the details of these reports.\n",
        "\n",
        "**Generic Prompts**\n",
        "\n",
        "The prompts are generic and have not been tailored to a specific content type. With further prompt engineering you'd expect to have more detailed metadata generate. For example: if most of your content is a specific type of report, doing prompt that captures specific items of that report you'll have richer metadata.\n"
      ],
      "metadata": {
        "id": "JfWKCeBX57X_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Before you start"
      ],
      "metadata": {
        "id": "uH327alV8xIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Requirements**\n",
        "\n",
        "Make sure you have the following resources in your GCP environment:\n",
        "\n",
        "\n",
        "*   Google cloud project with the APIs enabled;\n",
        "*   One Google cloud storage buckets - for the input files\n",
        "*   Firestore enabled and with a collection created.\n",
        "*   And your reports placed in the input bucket.\n",
        "\n",
        "Once you have this place, you'll be able to run the notebook.\n",
        "\n",
        "\n",
        "**Ingestion Pipeline**\n",
        "\n",
        "\n",
        "These are the relevant steps that the notebook will take you on:\n",
        "\n",
        "1.   Load The items from the input bucket;\n",
        "2.   Run the prompts for the reports;\n",
        "3.   Store the metadata;\n"
      ],
      "metadata": {
        "id": "bWWYHey78lcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "At the end of this pipeline you now have your GCS output bucket should have the following structure:\n",
        "\n",
        "```bash\n",
        "â”œâ”€â”€ file A\n",
        "â”‚   â”œâ”€â”€ splits\n",
        "â”‚   â”‚   â”œâ”€â”€ video split / key moment\n",
        "â”‚   â”‚   â”œâ”€â”€ wav - audio file for the split\n",
        "â”‚   â”‚   â”œâ”€â”€ json object with the transcription\n",
        "â”œâ”€â”€ file B\n",
        "â”‚   â”œâ”€â”€ splits\n",
        "â”‚   â”‚   â”œâ”€â”€ video split / key moment\n",
        "â”‚   â”‚   â”œâ”€â”€ wav - audio file for the split\n",
        "â”‚   â”‚   â”œâ”€â”€ json object with the transcription\n",
        "```\n",
        "The schema for the metadata for each of the asset types is shown in those sections.\n",
        "\n"
      ],
      "metadata": {
        "id": "cpV4vSRootGE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up ðŸ› "
      ],
      "metadata": {
        "id": "hMOTqxS6T9Yi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7Ln01eNJJmN"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade google-cloud-aiplatform google-cloud-speech firebase-admin librosa tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qGmdWacJLnU"
      },
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJ5O1-5gJP6O"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "# Additional authentication is required for Google Colab\n",
        "if \"google.colab\" in sys.modules:\n",
        "    # Authenticate user to Google Cloud\n",
        "    from google.colab import auth\n",
        "\n",
        "    auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variables"
      ],
      "metadata": {
        "id": "cJmhHE69UEJs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkC7zUXmJUN5"
      },
      "outputs": [],
      "source": [
        "PROJECT_ID = \"editorial-solaris\"  # @param {type:\"string\"}\n",
        "LOCATION = \"us-central1\"  # @param {type:\"string\"}\n",
        "MODEL = \"gemini-1.5-pro-001\" # @param {type:\"string\"}\n",
        "PIPELINE_INPUT_BUCKET = \"editorial-solaris-demo-pipeline-input\" #@param {type:\"string\"}\n",
        "INPUT_BUCKET=\"editorial-solaris-demo-input\" # @param {type:\"string\"}\n",
        "OUTPUT_BUCKET=\"gs://editorial-solaris-demo-output\" # @param {type:\"string\"}\n",
        "EMBEDDINGS_BUCKET = \"editorial-solaris-demo-embeddings\" # @param {type:\"string\"} editorial-solaris-demo-embeddings\n",
        "\n",
        "#db collection for firestore\n",
        "VIDEO_COLLECTION=\"video-demo\"   # @param {type:\"string\"}\n",
        "IMAGE_COLLECTION=\"images-demo\" # @param {type:\"string\"}\n",
        "AUDIO_COLLECTION=\"audio-demo\"  # @param {type:\"string\"}\n",
        "ARTICLE_COLLECTION=\"article-demo\"  # @param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports\n"
      ],
      "metadata": {
        "id": "Le6zvMW6Q4rf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#common import\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, FinishReason\n",
        "import vertexai.preview.generative_models as generative_models\n",
        "from vertexai.language_models import TextEmbeddingModel\n",
        "from google.cloud import storage\n",
        "import re\n",
        "import json\n",
        "import tqdm\n",
        "import os\n",
        "import io\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "\n",
        "#speech imports\n",
        "from google.api_core.client_options import ClientOptions\n",
        "from google.cloud.speech_v2 import SpeechClient\n",
        "from google.cloud.speech_v2.types import cloud_speech\n",
        "from google.api_core.client_options import ClientOptions\n",
        "import librosa\n",
        "\n",
        "#storage\n",
        "import firebase_admin\n",
        "from firebase_admin import firestore\n",
        "\n",
        "#threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Video\n",
        "from moviepy.editor import VideoFileClip\n",
        "from moviepy.editor import ImageClip"
      ],
      "metadata": {
        "id": "XIbKDszhQ6I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common Functions"
      ],
      "metadata": {
        "id": "wx9UzEKKUF0I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIBKRoL-Iby6"
      },
      "outputs": [],
      "source": [
        "def generate(prompt : list, model :str = MODEL, location :str = LOCATION) -> str:\n",
        "  vertexai.init(project=PROJECT_ID, location=location)\n",
        "\n",
        "  model = GenerativeModel(\n",
        "    model,\n",
        "  )\n",
        "\n",
        "  generation_config = {\n",
        "      \"max_output_tokens\": 8192,\n",
        "      \"temperature\": 1,\n",
        "      \"top_p\": 0.95,\n",
        "  }\n",
        "\n",
        "  safety_settings = {\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "  }\n",
        "\n",
        "  responses = model.generate_content(\n",
        "      prompt,\n",
        "      generation_config=generation_config,\n",
        "      safety_settings=safety_settings,\n",
        "      stream=False,\n",
        "  )\n",
        "  return responses.text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
        "    # The ID of your GCS bucket\n",
        "    # bucket_name = \"your-bucket-name\"\n",
        "\n",
        "    # The ID of your GCS object\n",
        "    # source_blob_name = \"storage-object-name\"\n",
        "\n",
        "    # The path to which the file should be downloaded\n",
        "    # destination_file_name = \"local/path/to/file\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    # Construct a client side representation of a blob.\n",
        "    # Note `Bucket.blob` differs from `Bucket.get_blob` as it doesn't retrieve\n",
        "    # any content from Google Cloud Storage. As we don't need additional data,\n",
        "    # using `Bucket.blob` is preferred here.\n",
        "    blob = bucket.blob(source_blob_name)\n",
        "    blob.download_to_filename(destination_file_name)\n",
        "\n",
        "    print(\n",
        "        \"Downloaded storage object {} from bucket {} to local file {}.\".format(\n",
        "            source_blob_name, bucket_name, destination_file_name\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "tz2EQESJZNwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_json_from_gcs(bucket_name, file_name):\n",
        "    \"\"\"Reads a JSON file from Google Cloud Storage into a Python dictionary.\n",
        "\n",
        "    Args:\n",
        "        bucket_name: The name of the GCS bucket.\n",
        "        file_name: The name of the JSON file within the bucket.\n",
        "\n",
        "    Returns:\n",
        "        A Python dictionary representing the JSON data, or None if an error occurred.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove 'gs://' prefix if present using replace()\n",
        "    bucket_name = bucket_name.replace(\"gs://\", \"\", 1)  # Replace only the first occurrence\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    try:\n",
        "        contents = blob.download_as_string()\n",
        "        data = json.loads(contents)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading JSON from GCS: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "taf94cMWmuE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_string_from_gcs(bucket_name, file_name):\n",
        "    \"\"\"Reads a text file from Google Cloud Storage into a Python string.\n",
        "\n",
        "    Args:\n",
        "        bucket_name: The name of the GCS bucket.\n",
        "        file_name: The name of the text file within the bucket.\n",
        "\n",
        "    Returns:\n",
        "        A string that has the data from the bucket, or None if an error occurred.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove 'gs://' prefix if present using replace()\n",
        "    bucket_name = bucket_name.replace(\"gs://\", \"\", 1)  # Replace only the first occurrence\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blob = bucket.blob(file_name)\n",
        "\n",
        "    try:\n",
        "        contents = blob.download_as_string()\n",
        "        return contents\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading text from GCS: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "uV-kFA7qtxl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_bucket_files_pd(bucket_name):\n",
        "    \"\"\"\n",
        "    Lists files in a GCS bucket with properties (name, size, updated time).\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Name of the GCS bucket.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing file properties.\n",
        "    \"\"\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs()\n",
        "\n",
        "    file_data = []\n",
        "    for blob in blobs:\n",
        "        file_data.append({\n",
        "            'file_name': blob.name,\n",
        "            'size': blob.size,\n",
        "              # Last updated timestamp\n",
        "            'type':blob.content_type,\n",
        "            'created': blob.time_created,\n",
        "            'updated': blob.updated,\n",
        "\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(file_data)\n",
        "    return df"
      ],
      "metadata": {
        "id": "mEKHf-tXk4d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def list_bucket_files(bucket_name):\n",
        "    \"\"\"\n",
        "    Lists files in a GCS bucket with properties (name, size, updated time).\n",
        "\n",
        "    Args:\n",
        "        bucket_name (str): Name of the GCS bucket.\n",
        "\n",
        "    Returns:\n",
        "        list: List of JSON objects containing file properties.\n",
        "    \"\"\"\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "    blobs = bucket.list_blobs()\n",
        "\n",
        "    file_data = []\n",
        "    for blob in blobs:\n",
        "        file_data.append({\n",
        "            'file_name': blob.name,\n",
        "            'size': blob.size,\n",
        "            'type': blob.content_type,\n",
        "            'created': blob.time_created,\n",
        "            'updated': blob.updated,\n",
        "        })\n",
        "\n",
        "    return file_data  # Return the list of JSON objects directly"
      ],
      "metadata": {
        "id": "zQJl4PbdL7P-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just removes the long running operations from the files\n",
        "def rename_transcription_files(bucket_name):\n",
        "    \"\"\"Renames files in a GCS bucket using a regular expression.\n",
        "\n",
        "    Args:\n",
        "        bucket_name: The name of the GCS bucket.\n",
        "    \"\"\"\n",
        "\n",
        "    # Remove 'gs://' prefix if present using replace()\n",
        "    bucket_name = bucket_name.replace(\"gs://\", \"\", 1)  # Replace only the first occurrence\n",
        "\n",
        "    storage_client = storage.Client()\n",
        "    bucket = storage_client.bucket(bucket_name)\n",
        "\n",
        "    blobs = bucket.list_blobs()  # Get a list of all blobs (files) in the bucket\n",
        "\n",
        "    for blob in blobs:\n",
        "        # Check if the file name matches the pattern\n",
        "        if re.match(r\".*_transcript_.*\\.json\", blob.name):\n",
        "            new_name = re.sub(r\"_transcript_.+\\.json\", \"_transcript.json\", blob.name)\n",
        "\n",
        "            # Only rename if the new name is different\n",
        "            if new_name != blob.name:\n",
        "                bucket.rename_blob(blob, new_name)\n",
        "                print(f\"Renamed {blob.name} to {new_name}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "NRgQBLgvkytH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metadata Generation ðŸ¤–\n",
        "\n"
      ],
      "metadata": {
        "id": "0O5qHzSiUI3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading assets\n",
        "\n",
        "Create a list of all the objects that are in the input bucket."
      ],
      "metadata": {
        "id": "TeosX91JUV5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_list = list_bucket_files(PIPELINE_INPUT_BUCKET)\n",
        "file_list"
      ],
      "metadata": {
        "id": "XlCI_k8-k9tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcription setup"
      ],
      "metadata": {
        "id": "gJowyYSN2WiF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imSfNmPO2bZL"
      },
      "source": [
        "First, you need to initiate a Recognizer which uses the Chirp model and transcribe the audio in English.\n",
        "\n",
        "See [the documentation](https://cloud.google.com/python/docs/reference/speech/latest/google.cloud.speech_v2.types.CreateRecognizerRequest) to learn more about how to configure the `CreateRecognizerRequest` request."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3w7BzVM2bZL"
      },
      "outputs": [],
      "source": [
        "client = SpeechClient(\n",
        "    client_options=ClientOptions(api_endpoint=f\"{LOCATION}-speech.googleapis.com\")\n",
        ")\n",
        "\n",
        "language_code = \"en-AU\"\n",
        "recognizer_id = f\"chirp-{language_code.lower()}-interview\"\n",
        "\n",
        "recognizer_request = cloud_speech.CreateRecognizerRequest(\n",
        "    parent=f\"projects/{PROJECT_ID}/locations/{LOCATION}\",\n",
        "    recognizer_id=recognizer_id,\n",
        "    recognizer=cloud_speech.Recognizer(\n",
        "        language_codes=[language_code],\n",
        "        model=\"chirp\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj_0e7sE2bZL"
      },
      "source": [
        "Then, you create a Speech-to-Text [Recognizer](https://cloud.google.com/speech-to-text/v2/docs/recognizers) that uses Chirp running a create operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vz3JsMEL2bZL"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    recognizer_name = f\"projects/{PROJECT_ID}/locations/{LOCATION}/recognizers/{recognizer_id}\"\n",
        "    recognizer = client.get_recognizer(name=recognizer_name)\n",
        "    print(f\"Recognizer '{recognizer_id}' already exists.\")\n",
        "except Exception:\n",
        "    print(f\"Recognizer '{recognizer_id}' does not exist.\")\n",
        "    create_operation = client.create_recognizer(request=recognizer_request)\n",
        "    recognizer = create_operation.result()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PRchiwJ2bZL"
      },
      "outputs": [],
      "source": [
        "recognizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Audio ðŸŽ§\n",
        "From the list of all input files, filter out the items that have audio."
      ],
      "metadata": {
        "id": "1Ea-I2b2E3SL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Field | Description |\n",
        "|---|---|\n",
        "| **Long Summary** | A detailed and comprehensive summary of the content. |\n",
        "| **Short Summary** | A concise and brief overview of the content. |\n",
        "| **Labels** | Keywords or tags associated with the content. |\n",
        "| **Transcript** | The transcript of the audio content. |"
      ],
      "metadata": {
        "id": "g9N8OmK1vrgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter out audio\n",
        "audio_list = []\n",
        "for file in file_list:\n",
        "  if 'audio' in file['type']:\n",
        "    audio_list.append(file)\n",
        "audio_list"
      ],
      "metadata": {
        "id": "U-uiazq8Gl-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary metadata ðŸ§™"
      ],
      "metadata": {
        "id": "XGY3jKfElhuo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_audio(prompt : list, model :str = MODEL) -> str:\n",
        "  vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "  model = GenerativeModel(\n",
        "    model,\n",
        "  )\n",
        "\n",
        "  generation_config = {\n",
        "      \"max_output_tokens\": 8192,\n",
        "      \"temperature\": 0.5,\n",
        "      \"top_p\": 0.95,\n",
        "  }\n",
        "\n",
        "  safety_settings = {\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "  }\n",
        "\n",
        "  responses = model.generate_content(\n",
        "      prompt,\n",
        "      generation_config=generation_config,\n",
        "      safety_settings=safety_settings,\n",
        "      stream=False,\n",
        "  )\n",
        "\n",
        "  return responses.text\n",
        "\n"
      ],
      "metadata": {
        "id": "5WLiCfOFljm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a image metadata\n",
        "def generate_metadata_audio(blob_uri: str, mime_type: str, model:str) -> str:\n",
        "  media_asset = Part.from_uri(\n",
        "      mime_type=mime_type,\n",
        "      uri=blob_uri)\n",
        "\n",
        "  prompt = \"\"\"SYSTEM:```You are a skilled podcast expert. You have a deep understanding of media. Your task is to analyze the provided audio and extract key information.```\n",
        "  INSTRUCTION: Please analyze the following video and provide long summary, short summary, subject topics.Please format your response as a JSON object with the given structure. Avoid any additional comment or text.\n",
        "  OUTPUT:```=\n",
        "  JSON\n",
        "  {\n",
        "    \"show_name\" : \"the name of the podcast show\"\n",
        "    \"short_summary\": \"[One paragraph summary of the content]\",\n",
        "    \"long_summary\": \"[two-three paragraph summary of the content]\",\n",
        "    \"subject_topics\" :\n",
        "      [     { \"topic\": \"[topic]\"}, { \"topic\": \"[topic]\"} ]\n",
        "  }```\n",
        "  \"\"\"\n",
        "\n",
        "  result_text = generate_audio(prompt=[media_asset, prompt], model = model )\n",
        "\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "9BTFrYQElmfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_audio(audio : json):\n",
        "  \"\"\"Processes a single row from the audio list.\"\"\"\n",
        "\n",
        "  blob_uri = f\"gs://{INPUT_BUCKET}/{audio['file_name']}\"\n",
        "  try:\n",
        "    result = generate_metadata_audio(blob_uri, audio['type'], MODEL)\n",
        "    response_text = re.sub(r\"json|```\", \"\", result)\n",
        "    audio['metadata'] = json.loads(response_text)\n",
        "    return result\n",
        "  except Exception as e:\n",
        "      print(f\"Error processing {audio['file_name']}: {e} \" )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_audio, audio) for audio in audio_list]\n",
        "\n",
        "    # You can remove the tqdm loop if you don't need progress updates\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('Audio Summary Metadata Generated')\n"
      ],
      "metadata": {
        "id": "K7XaoFqglz_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create the uris in the object's metadata\n",
        "for audio in audio_list:\n",
        "  audio['name'] = audio['file_name'].split('.')[0]\n",
        "  audio['gcs_uri'] = f\"gs://{INPUT_BUCKET}/{audio['file_name']}\"\n",
        "  formatted_text = json.dumps(audio, indent=4, default=str)\n",
        "  print(formatted_text)"
      ],
      "metadata": {
        "id": "S0wiH4BhmjwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcription"
      ],
      "metadata": {
        "id": "haonbLQ3yUWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Change for the audio configuration instead of the video sgement\n",
        "def process_segment_transcription(element, client, recognizer):\n",
        "    long_audio_uri = element['split_audio_uri']\n",
        "    directory_path = os.path.dirname(long_audio_uri)\n",
        "\n",
        "    long_audio_config = cloud_speech.RecognitionConfig(\n",
        "      features=cloud_speech.RecognitionFeatures(\n",
        "          enable_automatic_punctuation=True, enable_word_time_offsets=True\n",
        "      ),\n",
        "      auto_decoding_config={},\n",
        "    )\n",
        "\n",
        "\n",
        "    long_audio_request = cloud_speech.BatchRecognizeRequest(\n",
        "        recognizer=recognizer.name,\n",
        "        recognition_output_config={\n",
        "            \"gcs_output_config\": {\"uri\": directory_path}\n",
        "        },\n",
        "        files=[{\"config\": long_audio_config, \"uri\": long_audio_uri}],\n",
        "    )\n",
        "\n",
        "    long_audio_operation = client.batch_recognize(request=long_audio_request)\n",
        "    return long_audio_operation\n",
        "\n",
        "futures = []  # Create an empty list to store the results\n",
        "\n",
        "def process_video_segments_transcription(video):\n",
        "  with ThreadPoolExecutor() as executor:\n",
        "    futures = [executor.submit(process_segment_transcription, section, client, recognizer) for index, section in enumerate(video['sections'])]\n",
        "\n",
        "\n",
        "    # # waiting for threads to complete\n",
        "    # for _ in tqdm.tqdm(as_completed(futures), total=len(futures), desc=f\"Processing {video['file_name']}\"):\n",
        "    #     pass  # No need to process individual results here\n",
        "\n",
        "    for future in tqdm.tqdm(as_completed(futures), total=len(futures), desc=f\"Transcribing segments {video['file_name']}\"):\n",
        "      operation = future.result()  # Get the Operation object\n",
        "      # Wait for the operation to complete\n",
        "      response = operation.result()\n",
        "\n",
        "for audio in audio_list:\n",
        "  process_video_segments_transcription(audio)\n",
        "\n",
        "print(f\"Transcriptions done.\")"
      ],
      "metadata": {
        "id": "GfjS2IQ0ypO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove the long running operations ids from the transcriptions"
      ],
      "metadata": {
        "id": "ioPKpa0-yxkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rename_transcription_files(OUTPUT_BUCKET)"
      ],
      "metadata": {
        "id": "JBpaqLQUyxkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Images ðŸ“¸\n",
        "\n"
      ],
      "metadata": {
        "id": "yoOXJbIfFShD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each of the images we are simly creating some additional metadata to support future usage, like search or content generation. There's only one step: Run one prompt to get description, photo type, location, subject topics and person\n",
        "\n",
        "\n",
        "The resulting meatada has the following format:\n",
        "\n",
        "| Field | Description |\n",
        "|---|---|\n",
        "| **Description** | A textual description of the image content. |\n",
        "| **Photo Shot Type** | The type of shot used (e.g., close-up, landscape, portrait). |\n",
        "| **Location** | The place where the photo was taken. |\n",
        "| **Labels** | Keywords or tags associated with the image. |\n",
        "| **People** | Names or descriptions of people present in the image. |\n",
        "\n"
      ],
      "metadata": {
        "id": "9VEyOYBdvk5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter out videos\n",
        "image_list = []\n",
        "for file in file_list:\n",
        "  if 'image' in file['type']:\n",
        "    image_list.append(file)\n",
        "image_list"
      ],
      "metadata": {
        "id": "iE36IGZoFVNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image(prompt : list, model :str = MODEL) -> str:\n",
        "  vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "  model = GenerativeModel(\n",
        "    model,\n",
        "  )\n",
        "\n",
        "  generation_config = {\n",
        "      \"max_output_tokens\": 8192,\n",
        "      \"temperature\": 0.5,\n",
        "      \"top_p\": 0.95,\n",
        "  }\n",
        "\n",
        "  safety_settings = {\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "  }\n",
        "\n",
        "  responses = model.generate_content(\n",
        "      prompt,\n",
        "      generation_config=generation_config,\n",
        "      safety_settings=safety_settings,\n",
        "      stream=False,\n",
        "  )\n",
        "\n",
        "  return responses.text\n",
        "\n"
      ],
      "metadata": {
        "id": "JFevVJTjaTPe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a image metadata\n",
        "def generate_metadata_image(blob_uri: str, mime_type: str, model:str) -> str:\n",
        "  media_asset = Part.from_uri(\n",
        "      mime_type=mime_type,\n",
        "      uri=blob_uri)\n",
        "\n",
        "  prompt = \"\"\"SYSTEM:```You are a skilled image analysis expert. You have a deep understanding of media. Your task is to analyze the provided image and extract key information.```\n",
        "  INSTRUCTION: ```Please analyze the following image and provide a description, subject topics, photo type, persons.Please format your response as a JSON object with the given structure. Avoid any additional comment or text.```\n",
        "  OUTPUT:```=\n",
        "    JSON\n",
        "  {\n",
        "    \"description\": \"[A one line description that would support understanding the contents of the image]\",\n",
        "    \"photo_type\": \"[Type of angles used to take the photography]\",\n",
        "    \"location\" : \"A description of the location where the shot is taken, or if it is a known sight, its name\",\n",
        "    \"subject_topics\" :\n",
        "      [     { \"topic\": \"[topic]\"}, { \"topic\": \"[topic]\"} ]\n",
        "    \"persons\" :\n",
        "  [     { \"person\": \"[person1]\"}, { \"person\": \"[person2]\"} ]\n",
        "  }```\n",
        "  \"\"\"\n",
        "\n",
        "  result_text = generate_image(prompt=[media_asset, prompt], model = model )\n",
        "\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "n-Y4bg5FZZYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_image(image : json):\n",
        "  \"\"\"Processes a single row from the image list.\"\"\"\n",
        "\n",
        "  blob_uri = f\"gs://{INPUT_BUCKET}/{image['file_name']}\"\n",
        "  try:\n",
        "    result = generate_metadata_image(blob_uri, image['type'], MODEL)\n",
        "    # print(f\"Processing {image['file_name']} > {result}\")\n",
        "\n",
        "    response_text = re.sub(r\"json|```\", \"\", result)\n",
        "    image['metadata'] = json.loads(response_text)\n",
        "    return result\n",
        "  except Exception as e:\n",
        "      print(f\"Error processing {image['file_name']} {e}\")\n",
        "\n",
        "\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_image, image) for image in image_list]\n",
        "\n",
        "    # You can remove the tqdm loop if you don't need progress updates\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('Images Summary Metadata Generated')\n"
      ],
      "metadata": {
        "id": "eyBUUKbpaDgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in image_list:\n",
        "  image['name'] = image['file_name'].split('.')[0]\n",
        "  image['gcs_uri'] = f\"gs://{INPUT_BUCKET}/{image['file_name']}\"\n"
      ],
      "metadata": {
        "id": "jEWoWzUQagji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in image_list:\n",
        "  formatted_text = json.dumps(image, indent=4, default=str)\n",
        "  print(formatted_text)"
      ],
      "metadata": {
        "id": "i4i5kTqoavwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Videos ðŸŽ¥"
      ],
      "metadata": {
        "id": "wxH__sURUMPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section creates the metadata for the videos. The relevant steps are:\n",
        "\n",
        "1.   Create the summary metadata with Gemini\n",
        "2.   Ask Gemini to pick the timestamps where relevant thumbnails can be done, and create images out of it.\n",
        "3.   Detect the different key moments in the video, create the cuts of that video.\n",
        "4.   For each of the key moments, create a transcript of the audio.\n",
        "\n",
        "The metadata generated will follow this structure:\n",
        "\n",
        "\n",
        "| Field | Description |\n",
        "|---|---|\n",
        "| **Short Summary** | A concise and brief overview of the video content. |\n",
        "| **Long Summary** | A detailed and comprehensive summary of the video content. |\n",
        "| **Labels** | Keywords or tags associated with the video. |\n",
        "| **Thumbnails** | A list of thumbnails for the video. |\n",
        "| **Key Moments: Shot Type** | The type of shot used for the key moment (e.g., close-up, landscape). |\n",
        "| **Key Moments: Order** | The order or sequence of the key moment within the video. |\n",
        "| **Key Moments: Reason** | The reason why this moment is considered key or important. |\n",
        "| **Key Moments: Start Timestamp** | The timestamp in the video where the key moment starts. |\n",
        "| **Key Moments: End Timestamp** | The timestamp in the video where the key moment ends. |\n",
        "| **Key Moments: Transcript** | The speech-to-text transcript. |"
      ],
      "metadata": {
        "id": "2y2HKifmVK0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#filter out videos\n",
        "video_list = []\n",
        "for file in file_list:\n",
        "  if file['type'] == 'video/mp4':\n",
        "    video_list.append(file)\n",
        "video_list"
      ],
      "metadata": {
        "id": "EYQLXyEelSvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#copy files locally\n",
        "# Create the output folder if it doesn't exist\n",
        "output_folder = f\"content/\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "for video in video_list:\n",
        "  #copy file to notebook\n",
        "  destination_file_name = f\"content/{video['file_name']}\"\n",
        "  download_blob(PIPELINE_INPUT_BUCKET, video['file_name'], destination_file_name)\n"
      ],
      "metadata": {
        "id": "K8bkbiaZcHaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary metadata ðŸ§™\n",
        "This is the function that generates metadata for the asset types that are videos. It is currently not specific to the type of content (i,e: sports highlights versus a press conference highlight) but further refinment of the prompt that is different per content type will yield more accurate data\n"
      ],
      "metadata": {
        "id": "LxhAZyZLfoca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a summary of the video\n",
        "def generate_metadata_video_overview(blob_uri: str) -> str:\n",
        "  video1 = Part.from_uri(\n",
        "      mime_type=\"video/mp4\",\n",
        "      uri=blob_uri)\n",
        "\n",
        "  text1 = \"\"\"SYSTEM:```You are a skilled video analysis expert. You have a deep understanding of media. Your task is to analyze the provided video and extract key information.```\n",
        "  INSTRUCTION: ```Please analyze the following video and provide long summary, short summary, subject topics.Please format your response as a JSON object with the given structure. Avoid any additional comment or text.```\n",
        "  OUTPUT:```=\n",
        "  JSON\n",
        "  {\n",
        "    \"short_summary\": \"[One paragraph summary of the content]\",\n",
        "    \"long_summary\": \"[two-three paragraph summary of the content]\",\n",
        "    \"subject_topics\" :\n",
        "      [     { \"topic\": \"[topic]\"}, { \"topic\": \"[topic]\"} ]\n",
        "  }```\n",
        "  \"\"\"\n",
        "\n",
        "  result_text = generate(prompt=[video1, text1], model = 'gemini-1.5-pro-001' )\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "TuyKQMthftP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_row_summary(video):\n",
        "    \"\"\"Processes a single row from the video array.\"\"\"\n",
        "    try:\n",
        "        blob_uri = f\"gs://{PIPELINE_INPUT_BUCKET}/{video['file_name']}\"\n",
        "        result = generate_metadata_video_overview(blob_uri=blob_uri)\n",
        "        response_text = re.sub(r\"json|```\", \"\", result)\n",
        "        summary = json.loads(response_text)\n",
        "        video['summary'] = summary\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video['file_name']}: {e}\")\n",
        "\n",
        "# Number of worker threads (adjust based on your system and workload)\n",
        "num_workers = 4  # Or use os.cpu_count() for a reasonable default\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_row_summary, video) for video in video_list]\n",
        "\n",
        "    # futures = []\n",
        "    # for video in tqdm.tqdm(video_list, desc='Launching prompts'):\n",
        "    #   future = executor.submit(process_row_summary, video)\n",
        "    #   time.sleep(60)\n",
        "\n",
        "    #   futures.append(future)\n",
        "\n",
        "    # waiting for threads to complete\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('Video Summary Metadata Generated')"
      ],
      "metadata": {
        "id": "Hu-rY8ZoCitv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing the results of the top entry"
      ],
      "metadata": {
        "id": "E2Q6DViXrKn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_json = json.dumps(video_list[0], indent=4, default=str)\n",
        "print(formatted_json)\n"
      ],
      "metadata": {
        "id": "9UkB0Jk-qlw6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thumbnails â‡"
      ],
      "metadata": {
        "id": "uDoFJco-Ba4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_thumbnails(video_path, timestamps):\n",
        "    \"\"\"\n",
        "    Extracts thumbnails from a video at specified timestamps using moviepy.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): The path to the video file.\n",
        "        timestamps (list): A list of timestamps (in seconds) for the snapshots.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of snapshot images (PIL Image objects).\n",
        "    \"\"\"\n",
        "\n",
        "    clip = VideoFileClip(video_path)\n",
        "\n",
        "    snapshots = [clip.get_frame(t) for t in timestamps]\n",
        "\n",
        "    clip.close()\n",
        "\n",
        "    return snapshots\n"
      ],
      "metadata": {
        "id": "R_JYx7jlBhXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_metadata_video_thumbnails(blob_uri: str) -> str:\n",
        "  video1 = Part.from_uri(\n",
        "      mime_type=\"video/mp4\",\n",
        "      uri=blob_uri)\n",
        "  text1 = \"\"\"SYSTEM:```You are a skilled video analysis expert. You have a deep understanding of media and can accurately identify key moments in a video. Your task is to analyze the provided video and extract key thumbnails.```\n",
        "  INSTRUCTION: ```Give me the timestamp for 3 suitable thumbnails for this video that highlight the key moments.\n",
        "  Do not add any additional text.```\n",
        "  OUTPUT:```\n",
        "  JSON\n",
        "    \"thumbnails\": [\n",
        "      {\n",
        "        \"reason\": \"[Why this would be a suitable thumbnail for the video]\",\n",
        "        \"time\": \"[mm:ss]\",\n",
        "\n",
        "      },\n",
        "      {\n",
        "        \"reason\": \"[Why this would be a suitable thumbnail for the video]\",\n",
        "        \"time\": \"[mm:ss]\",\n",
        "      }\n",
        "    ]\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  result_text = generate(prompt=[video1, text1], model = 'gemini-1.5-pro-001' )\n",
        "\n",
        "  return result_text\n"
      ],
      "metadata": {
        "id": "lQxOTgl2sZzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_row_thumbnails(video):\n",
        "    \"\"\"Processes a single row from the video array.\"\"\"\n",
        "    try:\n",
        "        blob_uri = f\"gs://{PIPELINE_INPUT_BUCKET}/{video['file_name']}\"\n",
        "        result = generate_metadata_video_thumbnails(blob_uri=blob_uri)\n",
        "        response_text = re.sub(r\"json|```\", \"\", result)\n",
        "        print(response_text)\n",
        "        thumbnails_metadata = json.loads(response_text)\n",
        "        video['thumbnails'] = thumbnails_metadata['thumbnails']\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing video {video['file_name']}: {e}\")\n",
        "\n",
        "# Number of worker threads (adjust based on your system and workload)\n",
        "num_workers = 4  # Or use os.cpu_count() for a reasonable default\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_row_thumbnails, video) for video in video_list]\n",
        "\n",
        "    # waiting for threads to complete\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('Video Thumbnails Metadata Generated')"
      ],
      "metadata": {
        "id": "cMSDjspdtE9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774240cc-c4eb-4052-f96a-07ffb791b4ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:35<00:00, 155.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "{\n",
            "  \"thumbnails\": [\n",
            "    {\n",
            "      \"reason\": \"Shows the opening keynote title card for Google Cloud Next\",\n",
            "      \"time\": \"0:04\"\n",
            "    },\n",
            "    {\n",
            "      \"reason\": \"Shows Thomas Kurian, CEO of Google Cloud, entering the stage\",\n",
            "      \"time\": \"1:48\"\n",
            "    },\n",
            "    {\n",
            "      \"reason\": \"Shows Sundar Pichai, CEO of Google, on the screen addressing the audience\",\n",
            "      \"time\": \"3:37\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Video Thumbnails Metadata Generated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_json = json.dumps(video_list[2], indent=4, default=str)\n",
        "print(formatted_json)\n"
      ],
      "metadata": {
        "id": "bisovW5kwtDB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "11510eb3-8197-4253-9c2b-9fef97c28400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "list index out of range",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-92-f027b33eb47f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mformatted_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformatted_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def store_thumbnails(video_path, timestamps, output_prefix):\n",
        "    \"\"\"\n",
        "    Extracts thumbnails from a video at specified timestamps and uploads them to a GCS bucket as PNG files.\n",
        "\n",
        "    Args:\n",
        "        video_path (str): The path to the video file.\n",
        "        timestamps (list): A list of timestamps (in seconds) for the snapshots.\n",
        "        bucket_name (str): The name of the GCS bucket.\n",
        "        output_prefix (str): A prefix for the output filenames in the bucket (e.g., \"thumbnails/\").\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        snapshots = extract_thumbnails(video_path, timestamps)\n",
        "\n",
        "        for i, snapshot in enumerate(snapshots):\n",
        "            try:\n",
        "                img_clip = ImageClip(snapshot)\n",
        "                thumbnail_name = f\"{output_prefix}_thumbnail_{i}.png\"\n",
        "\n",
        "                #save to the local file\n",
        "                img_clip.save_frame(thumbnail_name)\n",
        "\n",
        "                img_clip.close()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing and uploading thumbnail {i}: {e}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting thumbnails from video {video_path}: {e}\")"
      ],
      "metadata": {
        "id": "cklBehDODi-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_time_to_seconds(time_str):\n",
        "  \"\"\"Converts a time string in the format 'M:SS' to seconds.\n",
        "\n",
        "  Args:\n",
        "    time_str: A string representing the time in the format 'M:SS'.\n",
        "\n",
        "  Returns:\n",
        "    An integer representing the total number of seconds.\n",
        "  \"\"\"\n",
        "\n",
        "  minutes, seconds = map(int, time_str.split(':'))\n",
        "  total_seconds = minutes * 60 + seconds\n",
        "  return total_seconds\n",
        "\n",
        "\n",
        "#copy files locally\n",
        "# Create the output folder if it doesn't exist\n",
        "output_folder = f\"thumbnails\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "for video in video_list:\n",
        "  #create the field name, which is the file_name without the file extension\n",
        "  video['name'] = video['file_name'].split('.')[0]\n",
        "\n",
        "\n",
        "for video in video_list:\n",
        "  file_name = f\"content/{video['file_name']}\"\n",
        "  if 'thumbnails' in video:\n",
        "    seconds_array = []\n",
        "    for thumbnail in video['thumbnails']:\n",
        "      seconds_array.append(convert_time_to_seconds(thumbnail['time']))\n",
        "      ouput_prefix = f\"thumbnails/{video['name']}\"\n",
        "      store_thumbnails(file_name, seconds_array, ouput_prefix)"
      ],
      "metadata": {
        "id": "UpGRhOYSD1mc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thumbnails_folder = f\"{OUTPUT_BUCKET}\"\n",
        "\n",
        "!gsutil -m cp -r thumbnails/ {thumbnails_folder}\n"
      ],
      "metadata": {
        "id": "0omfFRgJlPW8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59033fcf-aa79-4972-ccc7-228b8e99810c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://thumbnails/Google Cloud Next 24 - Keynote_thumbnail_2.png [Content-Type=image/png]...\n",
            "/ [0/3 files][    0.0 B/  1.8 MiB]   0% Done                                    \rCopying file://thumbnails/Google Cloud Next 24 - Keynote_thumbnail_0.png [Content-Type=image/png]...\n",
            "Copying file://thumbnails/Google Cloud Next 24 - Keynote_thumbnail_1.png [Content-Type=image/png]...\n",
            "/ [3/3 files][  1.8 MiB/  1.8 MiB] 100% Done                                    \n",
            "Operation completed over 3 objects/1.8 MiB.                                      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section Metadata ðŸ§™"
      ],
      "metadata": {
        "id": "3mPZOOnPfqpb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJ6sKkIuI53M"
      },
      "outputs": [],
      "source": [
        "def generate_metadata_video_sections(blob_uri: str) -> str:\n",
        "  video1 = Part.from_uri(\n",
        "      mime_type=\"video/mp4\",\n",
        "      uri=blob_uri)\n",
        "  text1 = \"\"\"SYSTEM:```You are a skilled video analysis expert. You have a deep understanding of media and can accurately identify key moments in a video. Your task is to analyze the provided video and extract all the highlight clips. For each clip, you need to classify the type of highlight and provide the precise start and end timestamps.```\n",
        "  INSTRUCTION: ```Please analyze the following video and provide a list of all the highlight clips with their type and timestamps. Also explain the reason why the selection of that particular timestamp has been made. Please format your response as a JSON object with the given structure. Make sure the audio is not truncated while suggesting the clips. Avoid any additional comment or text.```\n",
        "  OUTPUT:```\n",
        "  JSON\n",
        "  {\n",
        "    \"sections\": [\n",
        "      {\n",
        "        \"type\": \"[highlight type]\",\n",
        "        \"start_time\": \"[mm:ss]\",\n",
        "        \"end_time\": \"[mm:ss]\",\n",
        "        \"reason\" : \"\"\n",
        "      },\n",
        "      {\n",
        "        \"type\":\"[highlight type]\",\n",
        "        \"start_time\": \"[mm:ss]\",\n",
        "        \"end_time\": \"[mm:ss]\",\n",
        "        \"reason\" : \"\"\n",
        "      }\n",
        "    ]\n",
        "  }```\n",
        "  Please make sure the timestamps are accurate and reflect the precise start and end of each highlight clip.\"\"\"\n",
        "\n",
        "\n",
        "  result_text = generate(prompt=[video1, text1], model = MODEL )\n",
        "\n",
        "  return result_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def process_row(video):\n",
        "    \"\"\"Processes a single row from the video array, with error handling.\"\"\"\n",
        "    try:\n",
        "        blob_uri = f\"gs://{PIPELINE_INPUT_BUCKET}/{video['file_name']}\"\n",
        "        result = generate_metadata_video_sections(blob_uri=blob_uri)\n",
        "        response_text = re.sub(r\"json|```\", \"\", result)\n",
        "        print(response_text)\n",
        "        sections_metadata = json.loads(response_text)\n",
        "        video['sections'] = sections_metadata['sections']\n",
        "    except Exception as e:\n",
        "        # Log or print the error for debugging\n",
        "        print(f\"Error processing video sections: {video['file_name']}: {e}\")\n",
        "\n",
        "# Number of worker threads (adjust based on your system and workload)\n",
        "num_workers = 4  # Or use os.cpu_count() for a reasonable default\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_row, video) for video in video_list]\n",
        "\n",
        "    # waiting for threads to complete\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('Video Sections Metadata Generated')"
      ],
      "metadata": {
        "id": "-EK6NrsUrhyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "788f0972-2c39-4769-e8ea-072e8b60973d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [01:23<00:00, 83.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "{\n",
            "  \"sections\": [\n",
            "    {\n",
            "      \"type\": \"product announcement\",\n",
            "      \"start_time\": \"00:16\",\n",
            "      \"end_time\": \"01:35\",\n",
            "      \"reason\": \"The video showcases various examples of how Google AI is being used today.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"speaker introduction\",\n",
            "      \"start_time\": \"01:37\",\n",
            "      \"end_time\": \"02:06\",\n",
            "      \"reason\": \"The CEO of Google Cloud, Thomas Kurian, is introduced to the stage.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"speaker introduction\",\n",
            "      \"start_time\": \"03:26\",\n",
            "      \"end_time\": \"03:35\",\n",
            "      \"reason\": \"The CEO of Google and Alphabet, Sundar Pichai, is introduced on stage.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"product announcement\",\n",
            "      \"start_time\": \"04:49\",\n",
            "      \"end_time\": \"06:01\",\n",
            "      \"reason\": \"Sundar Pichai talks about Google's AI infrastructure and the introduction of Gemini 1.5 Pro.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"partnership announcement\",\n",
            "      \"start_time\": \"06:52\",\n",
            "      \"end_time\": \"07:27\",\n",
            "      \"reason\": \"Sundar Pichai announces partnerships with iconic companies like Goldman Sachs, Mercedes-Benz, Uber, and Palo Alto Networks.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"speaker introduction\",\n",
            "      \"start_time\": \"07:49\",\n",
            "      \"end_time\": \"07:54\",\n",
            "      \"reason\": \"The chairman and CEO of Goldman Sachs, David Solomon is welcomed to the stage.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"speaker introduction\",\n",
            "      \"start_time\": \"15:54\",\n",
            "      \"end_time\": \"16:05\",\n",
            "      \"reason\": \"VP/GM ML, Systems and Cloud AI at Google Cloud, Amin Vahdat is introduced on stage.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"product announcement\",\n",
            "      \"start_time\": \"17:07\",\n",
            "      \"end_time\": \"18:50\",\n",
            "      \"reason\": \"Amin Vahdat speaks about the introduction of Google Axion Processors, TPU v5p, Hyperdisk ML, and Dynamic Workload Scheduler.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"partnership announcement\",\n",
            "      \"start_time\": \"18:50\",\n",
            "      \"end_time\": \"20:51\",\n",
            "      \"reason\": \"Amin Vahdat talks about partnerships with Anthropic, Kakao Brain, and how 900% growth has been seen in GPU and TPU use with GKE in the past year.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"product announcement\",\n",
            "      \"start_time\": \"20:51\",\n",
            "      \"end_time\": \"21:28\",\n",
            "      \"reason\": \"Amin Vahdat speaks about bringing AI closer to where data is generated and how GDC offers vector search for sensitive data, and secret and top-secret accreditations.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"product announcement\",\n",
            "      \"start_time\": \"21:28\",\n",
            "      \"end_time\": \"21:59\",\n",
            "      \"reason\": \"Amin Vahdat announces the addition of NVIDIA GPU support and introduces new enhancements to the GPU portfolio, including general availability of NVIDIA's newest Grace Blackwell generation of GPUs coming to Google Cloud early in 2025. \"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"product announcement\",\n",
            "      \"start_time\": \"22:29\",\n",
            "      \"end_time\": \"23:16\",\n",
            "      \"reason\": \"Thomas Kurian announces the Google Axion Processor and highlights its features.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"product announcement\",\n",
            "      \"start_time\": \"23:16\",\n",
            "      \"end_time\": \"24:16\",\n",
            "      \"reason\": \"Thomas Kurian introduces Gemini 1.5 Pro and speaks about its enhanced capabilities.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"speaker introduction\",\n",
            "      \"start_time\": \"31:18\",\n",
            "      \"end_time\": \"31:34\",\n",
            "      \"reason\": \"The CEO of Mercedes-Benz, Ola KÃ¤llenius, is introduced on stage.\"\n",
            "    },\n",
            "    {\n",
            "      \"type\": \"product announcement\",\n",
            "      \"start_time\": \"31:34\",\n",
            "      \"end_time\": \"32:50\",\n",
            "      \"reason\": \"Ola KÃ¤llenius speaks about the partnership of Mercedes-Benz with Google and how the company is using Google Cloud AI to equip its cars.\"\n",
            "    }\n",
            "  ]\n",
            "}\n",
            "\n",
            "Video Sections Metadata Generated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Showing the result of the top video"
      ],
      "metadata": {
        "id": "4y-nFmhuPLh3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_json = json.dumps(video_list[0], indent=4, default=str)\n",
        "print(formatted_json)\n"
      ],
      "metadata": {
        "id": "3pWc-4kUsid2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98ec0071-8462-4a74-92ef-472520ce1b2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"file_name\": \"Google Cloud Next 24 - Keynote.mp4\",\n",
            "    \"size\": 614140803,\n",
            "    \"type\": \"video/mp4\",\n",
            "    \"created\": \"2024-11-14 12:56:10.137000+00:00\",\n",
            "    \"updated\": \"2024-11-14 12:56:10.137000+00:00\",\n",
            "    \"summary\": {\n",
            "        \"short_summary\": \"Thomas Kurian, CEO of Google Cloud, opens the Google Cloud Next keynote, emphasizing the company's advancements in AI infrastructure, generative AI tools, and partnerships. The event showcases how AI is transforming businesses and improving daily lives. Sundar Pichai, CEO of Google & Alphabet, highlights the incredible momentum in the cloud business, with a revenue run rate of $36 billion, five times the rate from five years ago. He emphasizes Google's deep investments in AI, particularly Gemini, a large language model with exceptional capabilities. David Solomon, Chairman and CEO of Goldman Sachs, discusses how Goldman Sachs utilizes AI to synthesize complex information efficiently, boosting developer productivity and enhancing client experiences. Ola K\\u00e4llenius, CEO of Mercedes-Benz, shares how Mercedes-Benz employs AI to create intuitive driver experiences. Amin Vahdat, VP/GM of ML, Systems, and Cloud AI at Google Cloud, discusses Google's AI infrastructure, including the AI Hypercomputer, designed for large-scale AI training and serving. He announces the general availability of A3 Mega VMs, powered by NVIDIA H100 Tensor Core GPUs, and the TPU v5p, Google's most powerful and scalable TPU. He also highlights the introduction of the Google Axion Processor, a custom Arm-based CPU, and enhancements to their storage products, such as Hyperdisk ML for AI inference and serving. Finally, Kurian emphasizes Vertex AI, a fast-growing enterprise AI platform, which offers a comprehensive set of tools for building, tuning, and deploying custom AI models. He highlights the integration with partners like Hugging Face and Replit, and the new features like prompt management and automatic side-by-side for evaluating and comparing models. Overall, the keynote emphasizes Google Cloud's commitment to empowering businesses with cutting-edge AI solutions.\",\n",
            "        \"long_summary\": \"The Google Cloud Next keynote commences with a captivating introduction showcasing the transformative power of AI in various domains, ranging from everyday tasks to complex scientific endeavors. Thomas Kurian, CEO of Google Cloud, welcomes the audience, emphasizing the company's achievements in expanding its infrastructure footprint and partnering with industry leaders. Sundar Pichai, CEO of Google & Alphabet, joins the stage, highlighting the remarkable growth of the cloud business, reaching a $36 billion annual revenue run rate. He attributes this success to Google's continuous efforts in AI development, specifically mentioning their latest language model, Gemini, which has revolutionized the AI landscape. \\nDavid Solomon, Chairman and CEO of Goldman Sachs, takes the stage, sharing insights into how Goldman Sachs leverages Google Cloud's AI platform to streamline processes, enhance client experiences, and boost developer productivity. Ola K\\u00e4llenius, CEO of Mercedes-Benz, follows, discussing the integration of AI into Mercedes-Benz vehicles, aiming to create intuitive and personalized driving experiences. \\nAmin Vahdat, VP/GM of ML, Systems, and Cloud AI at Google Cloud, delves into the technical advancements behind Google's AI infrastructure, introducing the AI Hypercomputer, a state-of-the-art system designed for large-scale AI training and serving. He announces the general availability of A3 Mega VMs and TPU v5p, further solidifying Google Cloud's leadership in AI infrastructure. Additionally, he unveils the upcoming Google Axion Processor, a custom-designed CPU aimed at optimizing performance and energy efficiency for data centers. The keynote concludes with Kurian emphasizing Vertex AI, Google Cloud's comprehensive platform for building, deploying, and managing AI models. He showcases the platform's features, including the Model Garden, which offers a vast selection of pre-trained models, and the introduction of new tools like prompt management and automatic side-by-side model evaluation. \",\n",
            "        \"subject_topics\": [\n",
            "            {\n",
            "                \"topic\": \"AI\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Generative AI\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Cloud Computing\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Google Cloud\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"AI Infrastructure\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Partnerships\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Vertex AI\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Gemini\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"TPU v5p\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Google Axion Processor\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"AI Agents\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Goldman Sachs\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Mercedes-Benz\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    \"thumbnails\": [\n",
            "        {\n",
            "            \"reason\": \"Shows the opening keynote title card for Google Cloud Next\",\n",
            "            \"time\": \"0:04\"\n",
            "        },\n",
            "        {\n",
            "            \"reason\": \"Shows Thomas Kurian, CEO of Google Cloud, entering the stage\",\n",
            "            \"time\": \"1:48\"\n",
            "        },\n",
            "        {\n",
            "            \"reason\": \"Shows Sundar Pichai, CEO of Google, on the screen addressing the audience\",\n",
            "            \"time\": \"3:37\"\n",
            "        }\n",
            "    ],\n",
            "    \"name\": \"Google Cloud Next 24 - Keynote\",\n",
            "    \"sections\": [\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"00:16\",\n",
            "            \"end_time\": \"01:35\",\n",
            "            \"reason\": \"The video showcases various examples of how Google AI is being used today.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"speaker introduction\",\n",
            "            \"start_time\": \"01:37\",\n",
            "            \"end_time\": \"02:06\",\n",
            "            \"reason\": \"The CEO of Google Cloud, Thomas Kurian, is introduced to the stage.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"speaker introduction\",\n",
            "            \"start_time\": \"03:26\",\n",
            "            \"end_time\": \"03:35\",\n",
            "            \"reason\": \"The CEO of Google and Alphabet, Sundar Pichai, is introduced on stage.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"04:49\",\n",
            "            \"end_time\": \"06:01\",\n",
            "            \"reason\": \"Sundar Pichai talks about Google's AI infrastructure and the introduction of Gemini 1.5 Pro.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"partnership announcement\",\n",
            "            \"start_time\": \"06:52\",\n",
            "            \"end_time\": \"07:27\",\n",
            "            \"reason\": \"Sundar Pichai announces partnerships with iconic companies like Goldman Sachs, Mercedes-Benz, Uber, and Palo Alto Networks.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"speaker introduction\",\n",
            "            \"start_time\": \"07:49\",\n",
            "            \"end_time\": \"07:54\",\n",
            "            \"reason\": \"The chairman and CEO of Goldman Sachs, David Solomon is welcomed to the stage.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"speaker introduction\",\n",
            "            \"start_time\": \"15:54\",\n",
            "            \"end_time\": \"16:05\",\n",
            "            \"reason\": \"VP/GM ML, Systems and Cloud AI at Google Cloud, Amin Vahdat is introduced on stage.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"17:07\",\n",
            "            \"end_time\": \"18:50\",\n",
            "            \"reason\": \"Amin Vahdat speaks about the introduction of Google Axion Processors, TPU v5p, Hyperdisk ML, and Dynamic Workload Scheduler.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"partnership announcement\",\n",
            "            \"start_time\": \"18:50\",\n",
            "            \"end_time\": \"20:51\",\n",
            "            \"reason\": \"Amin Vahdat talks about partnerships with Anthropic, Kakao Brain, and how 900% growth has been seen in GPU and TPU use with GKE in the past year.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"20:51\",\n",
            "            \"end_time\": \"21:28\",\n",
            "            \"reason\": \"Amin Vahdat speaks about bringing AI closer to where data is generated and how GDC offers vector search for sensitive data, and secret and top-secret accreditations.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"21:28\",\n",
            "            \"end_time\": \"21:59\",\n",
            "            \"reason\": \"Amin Vahdat announces the addition of NVIDIA GPU support and introduces new enhancements to the GPU portfolio, including general availability of NVIDIA's newest Grace Blackwell generation of GPUs coming to Google Cloud early in 2025. \"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"22:29\",\n",
            "            \"end_time\": \"23:16\",\n",
            "            \"reason\": \"Thomas Kurian announces the Google Axion Processor and highlights its features.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"23:16\",\n",
            "            \"end_time\": \"24:16\",\n",
            "            \"reason\": \"Thomas Kurian introduces Gemini 1.5 Pro and speaks about its enhanced capabilities.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"speaker introduction\",\n",
            "            \"start_time\": \"31:18\",\n",
            "            \"end_time\": \"31:34\",\n",
            "            \"reason\": \"The CEO of Mercedes-Benz, Ola K\\u00e4llenius, is introduced on stage.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"31:34\",\n",
            "            \"end_time\": \"32:50\",\n",
            "            \"reason\": \"Ola K\\u00e4llenius speaks about the partnership of Mercedes-Benz with Google and how the company is using Google Cloud AI to equip its cars.\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Section Clipping âœ‚"
      ],
      "metadata": {
        "id": "l-6PHwSCAt2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the folders in the notebook"
      ],
      "metadata": {
        "id": "__vp__gZbzKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create splits folder if it doesn't exist\n",
        "splits_folder = f\"content/splits\"\n",
        "if not os.path.exists(splits_folder):\n",
        "    os.makedirs(splits_folder)\n",
        "\n",
        "\n",
        "for video in video_list:\n",
        "  #create the splits folders\n",
        "  split_folder = f\"content/splits/{video['file_name'].split('.')[0]}\"\n",
        "  if not os.path.exists(split_folder):\n",
        "    os.makedirs(split_folder)\n"
      ],
      "metadata": {
        "id": "HfpRtAgvPoMi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create sections of the original video. Each thread should launch a video split, update the json object and output the file to the local notebook\n"
      ],
      "metadata": {
        "id": "2zQ-d9bxP7Rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import *\n",
        "\n",
        "\n",
        "def process_segment(video, index):\n",
        "    source_file_name = f\"content/{video['file_name']}\"\n",
        "    clip = VideoFileClip(source_file_name)\n",
        "\n",
        "    video_name = video['file_name'].split('.')[0]\n",
        "    clip_filename = f\"content/splits/{video_name}/{video_name}_split_{index}.mp4\"\n",
        "    audio_filename = f\"content/splits/{video_name}/{video_name}_split_{index}.wav\"\n",
        "\n",
        "    section_info = video['sections'][index]\n",
        "\n",
        "    #handling section for video\n",
        "    clip_subsection = clip.subclip(section_info[\"start_time\"], section_info[\"end_time\"])\n",
        "    clip_subsection.write_videofile(clip_filename, verbose=False, logger=None)\n",
        "\n",
        "    #handling audio split\n",
        "    audio_clip = clip_subsection.audio\n",
        "    audio_clip.write_audiofile(audio_filename, codec = 'pcm_s16le', verbose=False, logger=None)\n",
        "\n",
        "    #update the uri\n",
        "    gcs_video_uri = f\"{OUTPUT_BUCKET}/splits/{video_name}/{video_name}_split_{index}.mp4\"\n",
        "    gcs_audio_uri = f\"{OUTPUT_BUCKET}/splits/{video_name}/{video_name}_split_{index}.wav\"\n",
        "\n",
        "    video['sections'][index]['split_video_uri'] = gcs_video_uri\n",
        "    video['sections'][index]['split_audio_uri'] = gcs_audio_uri\n",
        "\n",
        "\n",
        "\n",
        "def process_video_segments(video):\n",
        "\n",
        "  #launching threads to do segment cut for one video\n",
        "  with ThreadPoolExecutor() as executor:\n",
        "    # futures = [executor.submit(process_segment, video) for video in video['sections']['highlights']]#TODO: unecessary sections in json object\n",
        "    if 'sections' in video:\n",
        "      futures = [executor.submit(process_segment, video, index) for index, row in enumerate(video['sections'])]\n",
        "      # waiting for threads to complete\n",
        "      for _ in tqdm.tqdm(as_completed(futures), total=len(futures), desc = f\"Processing {video['file_name']}\"):\n",
        "          pass  # No need to process individual results here\n",
        "\n",
        "\n",
        "for video in video_list:\n",
        "  process_video_segments(video)\n",
        "\n",
        "\n",
        "print('Video Sections Metadata Generated')"
      ],
      "metadata": {
        "id": "yk6MPJCbQ-Vw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f41faa8-aa86-4613-8d25-9be1664cfa14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Google Cloud Next 24 - Keynote.mp4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [20:14<00:00, 80.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video Sections Metadata Generated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for video in video_list:\n",
        "  # print(video['file_name'])\n",
        "  string = json.dumps(video, indent=4, default=str)\n",
        "  print(string)"
      ],
      "metadata": {
        "id": "818d3a3jeoy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Copy the files from the local folder to the GCS Bucket"
      ],
      "metadata": {
        "id": "RzhQ8573BfRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Move all files in content/splits to a gcs bucket using gsutils\n",
        "!gsutil -m cp -r content/splits {OUTPUT_BUCKET}\n"
      ],
      "metadata": {
        "id": "NSCwQCbs3PlB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a39964cf-4329-47e2-cce4-228e38eeec40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_8.mp4 [Content-Type=video/mp4]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_7.mp4 [Content-Type=video/mp4]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_10.mp4 [Content-Type=video/mp4]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_0.mp4 [Content-Type=video/mp4]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_7.wav [Content-Type=audio/x-wav]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \r/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_14.wav [Content-Type=audio/x-wav]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_12.mp4 [Content-Type=video/mp4]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \r/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_4.wav [Content-Type=audio/x-wav]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_11.wav [Content-Type=audio/x-wav]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_2.wav [Content-Type=audio/x-wav]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \r/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \r/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_13.mp4 [Content-Type=video/mp4]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_9.wav [Content-Type=audio/x-wav]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_5.mp4 [Content-Type=video/mp4]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_8.wav [Content-Type=audio/x-wav]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_12.wav [Content-Type=audio/x-wav]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_11.mp4 [Content-Type=video/mp4]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_2.mp4 [Content-Type=video/mp4]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_9.mp4 [Content-Type=video/mp4]...\n",
            "/ [0/30 files][    0.0 B/353.1 MiB]   0% Done                                   \rCopying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_1.mp4 [Content-Type=video/mp4]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_6.mp4 [Content-Type=video/mp4]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_1.wav [Content-Type=audio/x-wav]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_0.wav [Content-Type=audio/x-wav]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_6.wav [Content-Type=audio/x-wav]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_10.wav [Content-Type=audio/x-wav]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_5.wav [Content-Type=audio/x-wav]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_4.mp4 [Content-Type=video/mp4]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_3.wav [Content-Type=audio/x-wav]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_14.mp4 [Content-Type=video/mp4]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_13.wav [Content-Type=audio/x-wav]...\n",
            "Copying file://content/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_3.mp4 [Content-Type=video/mp4]...\n",
            "\\ [30/30 files][353.1 MiB/353.1 MiB] 100% Done                                  \n",
            "Operation completed over 30 objects/353.1 MiB.                                   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scene description"
      ],
      "metadata": {
        "id": "wKbIOjRV2PXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_metadata_video_sections_descriptions(blob_uri: str) -> str:\n",
        "  video1 = Part.from_uri(\n",
        "      mime_type=\"video/mp4\",\n",
        "      uri=blob_uri)\n",
        "  text1 = \"\"\"SYSTEM:```You are a skilled video analysis expert. You have a deep understanding of media and can accurately identify key moments in a video. Your task is to analyze the provided video and extract all the highlight clips. For each clip, you need to classify the type of highlight and provide the precise start and end timestamps.```\n",
        "  INSTRUCTION: ```Generate a scene description for somebody that is visually impaired.\n",
        "    Format it as something that will be dictated.\n",
        "    In each scene capture people's movement and actions.\n",
        "    [description] should be 1-2 sentences long\n",
        "\n",
        "  Output should be:\n",
        "  ```\n",
        "  OUTPUT:```\n",
        "  JSON\n",
        "  {\n",
        "    \"descriptions\" :\n",
        "      [\n",
        "\t\t{ \"start_timestamp\": \"[timecode to start the description]\", \"description\": [short description of the scene]},\n",
        "\t\t{ \"start_timestamp\": \"[timecode to start the description]\", \"description\": [short description of the scene]},\n",
        "\t]\n",
        "  }```\n",
        "  \"\"\"\n",
        "\n",
        "\n",
        "  result_text = generate(prompt=[video1, text1], model = MODEL )\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "J37MAC5qR94e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for video in video_list:\n",
        "  if 'sections' in video:\n",
        "    for index, segment in enumerate(video['sections']):\n",
        "      video_name = video['file_name'].split('.')[0]\n",
        "      segment['split_transcription_uri'] = f\"{OUTPUT_BUCKET}/splits/{video_name}/{video_name}_split_{index}_transcript.json\"\n"
      ],
      "metadata": {
        "id": "b3SaWyrt2R60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transcription ðŸ““\n"
      ],
      "metadata": {
        "id": "_4ifUYKnTsmh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ccreate the batch long running request and wait for the results"
      ],
      "metadata": {
        "id": "XgKZ3wTQofak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def process_segment_transcription(element, client, recognizer):\n",
        "    long_audio_uri = element['split_audio_uri']\n",
        "    directory_path = os.path.dirname(long_audio_uri)\n",
        "\n",
        "    long_audio_config = cloud_speech.RecognitionConfig(\n",
        "      features=cloud_speech.RecognitionFeatures(\n",
        "          enable_automatic_punctuation=True, enable_word_time_offsets=True\n",
        "      ),\n",
        "      auto_decoding_config={},\n",
        "    )\n",
        "\n",
        "\n",
        "    long_audio_request = cloud_speech.BatchRecognizeRequest(\n",
        "        recognizer=recognizer.name,\n",
        "        recognition_output_config={\n",
        "            \"gcs_output_config\": {\"uri\": directory_path}\n",
        "        },\n",
        "        files=[{\"config\": long_audio_config, \"uri\": long_audio_uri}],\n",
        "    )\n",
        "\n",
        "    long_audio_operation = client.batch_recognize(request=long_audio_request)\n",
        "    return long_audio_operation\n",
        "\n",
        "futures = []  # Create an empty list to store the results\n",
        "\n",
        "def process_video_segments_transcription(video):\n",
        "  with ThreadPoolExecutor() as executor:\n",
        "    if 'sections' in video:\n",
        "      futures = [executor.submit(process_segment_transcription, section, client, recognizer) for index, section in enumerate(video['sections'])]\n",
        "\n",
        "\n",
        "      # # waiting for threads to complete\n",
        "      # for _ in tqdm.tqdm(as_completed(futures), total=len(futures), desc=f\"Processing {video['file_name']}\"):\n",
        "      #     pass  # No need to process individual results here\n",
        "\n",
        "      for future in tqdm.tqdm(as_completed(futures), total=len(futures), desc=f\"Transcribing segments {video['file_name']}\"):\n",
        "        operation = future.result()  # Get the Operation object\n",
        "        # Wait for the operation to complete\n",
        "        response = operation.result()\n",
        "\n",
        "for video in video_list:\n",
        "  process_video_segments_transcription(video)\n",
        "\n",
        "print(f\"Transcriptions done.\")"
      ],
      "metadata": {
        "id": "JmCtThzcaUl7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6faa566e-de51-45ce-c329-27d19c11042e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Transcribing segments Google Cloud Next 24 - Keynote.mp4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:13<00:00,  1.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcriptions done.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "remove the long running operations ids from the transcriptions"
      ],
      "metadata": {
        "id": "isecFPoiokJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rename_transcription_files(OUTPUT_BUCKET)"
      ],
      "metadata": {
        "id": "oddhgeKCfgCO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29fc903a-b955-44f5-b69f-85083b0b7f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_0_transcript_6738490d-0000-25f3-bcbc-30fd38137ee4.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_0_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_10_transcript_67380441-0000-2808-a9f9-f403043cbb2c.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_10_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_11_transcript_67380443-0000-2808-a9f9-f403043cbb2c.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_11_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_12_transcript_67381c30-0000-2b15-accc-30fd3816ef2c.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_12_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_13_transcript_6738190e-0000-224e-bc3d-883d24ffa974.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_13_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_14_transcript_67384918-0000-25f3-bcbc-30fd38137ee4.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_14_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_1_transcript_6738490c-0000-25f3-bcbc-30fd38137ee4.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_1_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_2_transcript_67381c27-0000-2b15-accc-30fd3816ef2c.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_2_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_3_transcript_67380434-0000-2808-a9f9-f403043cbb2c.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_3_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_4_transcript_67381906-0000-224e-bc3d-883d24ffa974.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_4_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_5_transcript_67380435-0000-2808-a9f9-f403043cbb2c.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_5_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_6_transcript_67381901-0000-224e-bc3d-883d24ffa974.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_6_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_7_transcript_67381c29-0000-2b15-accc-30fd3816ef2c.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_7_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_8_transcript_6738491a-0000-25f3-bcbc-30fd38137ee4.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_8_transcript.json\n",
            "Renamed splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_9_transcript_6737e08b-0000-2a71-a104-089e082c7774.json to splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_9_transcript.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add the transcriptions information to the overall object.\n",
        "\n",
        "\n",
        "\n",
        "1.   Add the transcription_uri\n",
        "2.   put the accepted transcript in the json object\n",
        "\n"
      ],
      "metadata": {
        "id": "_hgFq4gEopss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for video in video_list:\n",
        "  if 'sections' in video:\n",
        "    for index, segment in enumerate(video['sections']):\n",
        "      video_name = video['file_name'].split('.')[0]\n",
        "      segment['split_transcription_uri'] = f\"{OUTPUT_BUCKET}/splits/{video_name}/{video_name}_split_{index}_transcript.json\"\n",
        "      #read transcription into memory\n",
        "      file_path = segment['split_transcription_uri'].replace(f\"{OUTPUT_BUCKET}/\", \"\")\n",
        "\n",
        "      transcription_results = read_json_from_gcs(OUTPUT_BUCKET, file_path)\n",
        "      final_transcription = \"\"\n",
        "\n",
        "      # Extract and concatenate transcripts\n",
        "      transcripts = []\n",
        "      for result in transcription_results['results']:\n",
        "        if 'alternatives' in result:\n",
        "          for alternative in result['alternatives']:\n",
        "              transcripts.append(alternative['transcript'])\n",
        "      if transcripts:\n",
        "        concatenated_transcript = ' '.join(transcripts)\n",
        "        segment['transcription'] = concatenated_transcript"
      ],
      "metadata": {
        "id": "hELl93irjbbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for video in video_list[:1]:\n",
        "  string = json.dumps(video, indent=4, default=str)\n",
        "  print(string)"
      ],
      "metadata": {
        "id": "rC4eYhTfo2QL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93004999-9d65-4ce5-f9ff-6c9792b71eea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"file_name\": \"Google Cloud Next 24 - Keynote.mp4\",\n",
            "    \"size\": 614140803,\n",
            "    \"type\": \"video/mp4\",\n",
            "    \"created\": \"2024-11-14 12:56:10.137000+00:00\",\n",
            "    \"updated\": \"2024-11-14 12:56:10.137000+00:00\",\n",
            "    \"summary\": {\n",
            "        \"short_summary\": \"Thomas Kurian, CEO of Google Cloud, opens the Google Cloud Next keynote, emphasizing the company's advancements in AI infrastructure, generative AI tools, and partnerships. The event showcases how AI is transforming businesses and improving daily lives. Sundar Pichai, CEO of Google & Alphabet, highlights the incredible momentum in the cloud business, with a revenue run rate of $36 billion, five times the rate from five years ago. He emphasizes Google's deep investments in AI, particularly Gemini, a large language model with exceptional capabilities. David Solomon, Chairman and CEO of Goldman Sachs, discusses how Goldman Sachs utilizes AI to synthesize complex information efficiently, boosting developer productivity and enhancing client experiences. Ola K\\u00e4llenius, CEO of Mercedes-Benz, shares how Mercedes-Benz employs AI to create intuitive driver experiences. Amin Vahdat, VP/GM of ML, Systems, and Cloud AI at Google Cloud, discusses Google's AI infrastructure, including the AI Hypercomputer, designed for large-scale AI training and serving. He announces the general availability of A3 Mega VMs, powered by NVIDIA H100 Tensor Core GPUs, and the TPU v5p, Google's most powerful and scalable TPU. He also highlights the introduction of the Google Axion Processor, a custom Arm-based CPU, and enhancements to their storage products, such as Hyperdisk ML for AI inference and serving. Finally, Kurian emphasizes Vertex AI, a fast-growing enterprise AI platform, which offers a comprehensive set of tools for building, tuning, and deploying custom AI models. He highlights the integration with partners like Hugging Face and Replit, and the new features like prompt management and automatic side-by-side for evaluating and comparing models. Overall, the keynote emphasizes Google Cloud's commitment to empowering businesses with cutting-edge AI solutions.\",\n",
            "        \"long_summary\": \"The Google Cloud Next keynote commences with a captivating introduction showcasing the transformative power of AI in various domains, ranging from everyday tasks to complex scientific endeavors. Thomas Kurian, CEO of Google Cloud, welcomes the audience, emphasizing the company's achievements in expanding its infrastructure footprint and partnering with industry leaders. Sundar Pichai, CEO of Google & Alphabet, joins the stage, highlighting the remarkable growth of the cloud business, reaching a $36 billion annual revenue run rate. He attributes this success to Google's continuous efforts in AI development, specifically mentioning their latest language model, Gemini, which has revolutionized the AI landscape. \\nDavid Solomon, Chairman and CEO of Goldman Sachs, takes the stage, sharing insights into how Goldman Sachs leverages Google Cloud's AI platform to streamline processes, enhance client experiences, and boost developer productivity. Ola K\\u00e4llenius, CEO of Mercedes-Benz, follows, discussing the integration of AI into Mercedes-Benz vehicles, aiming to create intuitive and personalized driving experiences. \\nAmin Vahdat, VP/GM of ML, Systems, and Cloud AI at Google Cloud, delves into the technical advancements behind Google's AI infrastructure, introducing the AI Hypercomputer, a state-of-the-art system designed for large-scale AI training and serving. He announces the general availability of A3 Mega VMs and TPU v5p, further solidifying Google Cloud's leadership in AI infrastructure. Additionally, he unveils the upcoming Google Axion Processor, a custom-designed CPU aimed at optimizing performance and energy efficiency for data centers. The keynote concludes with Kurian emphasizing Vertex AI, Google Cloud's comprehensive platform for building, deploying, and managing AI models. He showcases the platform's features, including the Model Garden, which offers a vast selection of pre-trained models, and the introduction of new tools like prompt management and automatic side-by-side model evaluation. \",\n",
            "        \"subject_topics\": [\n",
            "            {\n",
            "                \"topic\": \"AI\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Generative AI\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Cloud Computing\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Google Cloud\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"AI Infrastructure\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Partnerships\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Vertex AI\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Gemini\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"TPU v5p\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Google Axion Processor\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"AI Agents\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Goldman Sachs\"\n",
            "            },\n",
            "            {\n",
            "                \"topic\": \"Mercedes-Benz\"\n",
            "            }\n",
            "        ]\n",
            "    },\n",
            "    \"thumbnails\": [\n",
            "        {\n",
            "            \"reason\": \"Shows the opening keynote title card for Google Cloud Next\",\n",
            "            \"time\": \"0:04\"\n",
            "        },\n",
            "        {\n",
            "            \"reason\": \"Shows Thomas Kurian, CEO of Google Cloud, entering the stage\",\n",
            "            \"time\": \"1:48\"\n",
            "        },\n",
            "        {\n",
            "            \"reason\": \"Shows Sundar Pichai, CEO of Google, on the screen addressing the audience\",\n",
            "            \"time\": \"3:37\"\n",
            "        }\n",
            "    ],\n",
            "    \"name\": \"Google Cloud Next 24 - Keynote\",\n",
            "    \"sections\": [\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"00:16\",\n",
            "            \"end_time\": \"01:35\",\n",
            "            \"reason\": \"The video showcases various examples of how Google AI is being used today.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_0.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_0.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_0_transcript.json\",\n",
            "            \"transcription\": \" And sure, we can ask for answers to life's tough questions, but today we can also ask it to do tough things, like shift that wood more efficiently, use satellites to reduce methane emissions, turn DNA into code to make drout resistant corn. Oh, cool. It can spot and fill pots. Spot disease earlier, spot asteroids to protect earth. It can create apps with just words.  in just hours. Today, Google AI can scan 100 thousand lines of code in 2 minutes to spot and fix bugs. It can translate from code to code and is one step closer to speaking a thousand languages. Even where. Today, AI impacts lives for the better and understands the world the way you do. Our AI spots threat seven times faster. Understand speech sentiment, helps prevent vision loss, predicts weather 10 days out.  2 minutes, translates thousands of pages in seconds, detects lead pipes to keep drinking water clean, eliminates paperwork so cares more human, generates from text, visuals, audio and video, creates entire worlds from your imagination, creates this thing, and that thing, a new thing, the new way, it's cloud.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"speaker introduction\",\n",
            "            \"start_time\": \"01:37\",\n",
            "            \"end_time\": \"02:06\",\n",
            "            \"reason\": \"The CEO of Google Cloud, Thomas Kurian, is introduced to the stage.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_1.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_1.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_1_transcript.json\",\n",
            "            \"transcription\": \" Please welcome our CEO of Google Cloud, Thomas Curian. Wow.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"speaker introduction\",\n",
            "            \"start_time\": \"03:26\",\n",
            "            \"end_time\": \"03:35\",\n",
            "            \"reason\": \"The CEO of Google and Alphabet, Sundar Pichai, is introduced on stage.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_2.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_2.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_2_transcript.json\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"04:49\",\n",
            "            \"end_time\": \"06:01\",\n",
            "            \"reason\": \"Sundar Pichai talks about Google's AI infrastructure and the introduction of Gemini 1.5 Pro.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_3.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_3.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_3_transcript.json\",\n",
            "            \"transcription\": \" use now in the fifth generation, these advancements have helped customers train and serve cutting edge language models. these investments put us at the forefront of the AI platform shift and we are proud that today more than 60% of f generative AI startups and nearly 90% of genAI unicorns are google cloud customers. we also continue to build capable AI models to make products like search, maps and android radically more.  in December we took our next big step with Gemini, our largest and most capable model yet. We've been bringing it to our products and to enterprises and developers through our APIs. We've already introduced our next generation Gemini 1.5 Pro. It's been in private preview in Vertex AI. 1.5 Pro shows dramatically enhanced performance and includes a breakthrough in long context understanding. That means it can run.  one million tokens of information consistently, opening up new possibilities for enterprises to create, discover and build using AI. this also geminies multiodal capability.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"partnership announcement\",\n",
            "            \"start_time\": \"06:52\",\n",
            "            \"end_time\": \"07:27\",\n",
            "            \"reason\": \"Sundar Pichai announces partnerships with iconic companies like Goldman Sachs, Mercedes-Benz, Uber, and Palo Alto Networks.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_4.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_4.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_4_transcript.json\",\n",
            "            \"transcription\": \" David Solomon, who will share how Goldman Sacks is using AI to synthesize complex information quickly without compromising quality. All Kelleners who share how Mercedes-Benz is building intuitive new experiences for drivers. Dara Kasaf Shahi will share how Uber is using AI to empower employees and improve customer experiences and Nikesh Arora who will share how Palowalto networks.  is using generative AI to find the security needle in the data h stack.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"speaker introduction\",\n",
            "            \"start_time\": \"07:49\",\n",
            "            \"end_time\": \"07:54\",\n",
            "            \"reason\": \"The chairman and CEO of Goldman Sachs, David Solomon is welcomed to the stage.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_5.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_5.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_5_transcript.json\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"speaker introduction\",\n",
            "            \"start_time\": \"15:54\",\n",
            "            \"end_time\": \"16:05\",\n",
            "            \"reason\": \"VP/GM ML, Systems and Cloud AI at Google Cloud, Amin Vahdat is introduced on stage.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_6.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_6.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_6_transcript.json\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"17:07\",\n",
            "            \"end_time\": \"18:50\",\n",
            "            \"reason\": \"Amin Vahdat speaks about the introduction of Google Axion Processors, TPU v5p, Hyperdisk ML, and Dynamic Workload Scheduler.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_7.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_7.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_7_transcript.json\",\n",
            "            \"transcription\": \" enhancements at every layer of the stack: first, we have the leading portfolio of performance optimized accelerators, including Google Design TPUs and Nvidia GPUs. Today, we're announcing several enhancements to our GPU portfolio, including the upcoming general availability of A3 Mega powered by Nvidia H100 tensor core GPUs, with twice the network bandwidth per GPU, compared to A3 instances.  \\\"the rate of progress with Nvidia GPUs is truly astonishing. Today we are announcing support for Nvidia's newest grace Blackwell generation of GPUs coming to Google Cloud early in 2025. The Nvidia B200 and GB 200 chips are both powered by next generation Nvidia networking. GB-200 will be among the most advanced chips on the planet and will require liquid cooling to operate at peak efficiency.  Finally, we're also announcing the general availability of TPU V5P, our most powerful and scalable TPU, our latest generation TPU pod consists of 8,960 chips interconnected to support the largest scale ML training and serving. Our TPU V5p pods have 4x the compute capacity per pod compared to our previous generation, and with TPU Multice technology, you can't  simultaneously and transparently run across multiple pods for your most demanding workloads. In fact, we train and serve our latest Gemini models on our TPUs.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"partnership announcement\",\n",
            "            \"start_time\": \"18:50\",\n",
            "            \"end_time\": \"20:51\",\n",
            "            \"reason\": \"Amin Vahdat talks about partnerships with Anthropic, Kakao Brain, and how 900% growth has been seen in GPU and TPU use with GKE in the past year.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_8.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_8.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_8_transcript.json\",\n",
            "            \"transcription\": \" To improve AI training and inference speeds, we've added a number of enhancements to our storage products. Specifically, today we're accelerating inference with hyperdisk ML. Our next generation block storage service optimized for AI inference and serving workloads. It accelerates model load times up to 11.9x compared to common alternatives and offers over 100 times greater throughput per volume versus competitors and for ML developers,  We continue to optimize open and leading ML frameworks like Jacks, P torch, and Tensorflow for use with both vertex and GKE. Jax, an open source partnership that includes Nvidia, AMD, Intel, and many others has emerged as the leading framework for the most advanced ML practitioners. It allows developers to focus on the training logic declaratively, rather than partitioning code manually. Our XLA compiler then generates.  code for a range of GPU and TPU targets. In fact, over the last year, the use of GPUs and TPUs on GKE has grown more than 900%. Finally, with data intensive workloads like AI, efficient resource management is vital. Today we are launching two new options for dynamic workload scheduler, our resource management and job scheduling tool. Calendar mode for start time assurance and flex start for optimized economics. In combination,  These make a massive impact on system throughput and on cost efficiency. Overall, we find that our AI hypercomputer can run with more than twice the effective efficiency relative to baseline hardware only techniques. Leading companies like Anthropic have trained and served models on our AI hypercomputer and Cacao Brain, part of Korean technology company Cacao Group has built a large-scale AI language model that is the largest Korean language specific LLM in the market with 66 billion.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"20:51\",\n",
            "            \"end_time\": \"21:28\",\n",
            "            \"reason\": \"Amin Vahdat speaks about bringing AI closer to where data is generated and how GDC offers vector search for sensitive data, and secret and top-secret accreditations.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_9.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_9.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_9_transcript.json\",\n",
            "            \"transcription\": \" They've also developed a text image generator called Carlo. We are also bringing AI closer to where the data is being generated and consumed to the edge, to air gapped environments, to google sovereign clouds and cross cloud through Google distributed Cloud. Today we are announcing that we are adding Nvidia GPU support, adding GKE capabilities for AI workloads and enabling a variety of open AI models to run on GDC, including Gemma and  Lama, we are bringing the power of vector search to allow search and information retrieval applications for private and sensitive data.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"21:28\",\n",
            "            \"end_time\": \"21:59\",\n",
            "            \"reason\": \"Amin Vahdat announces the addition of NVIDIA GPU support and introduces new enhancements to the GPU portfolio, including general availability of NVIDIA's newest Grace Blackwell generation of GPUs coming to Google Cloud early in 2025. \",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_10.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_10.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_10_transcript.json\",\n",
            "            \"transcription\": \" For your most stringent regulatory requirements, we deliver fully air gapped configurations with local operators of your choice. You have complete control of your data, including location, encryption, and access control, and GDC now has both secret and top secret accreditations. Mobile Provider Orange operates in 26 different countries where local data must be kept in each country. They are using AI on GDC to improve network performance and to deliver super.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"22:29\",\n",
            "            \"end_time\": \"23:16\",\n",
            "            \"reason\": \"Thomas Kurian announces the Google Axion Processor and highlights its features.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_11.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_11.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_11_transcript.json\",\n",
            "            \"transcription\": \" We're delighted to announce the Google Action processor, it's our first custom arm based CPU designed for the data center and will be available in preview later this year. Google axion. Google action combines Google's.  expertise with RM's latest compute core designs to better deliver up to 50% better performance and up to 60% better energy efficiency than comparable current generation X86 based VMs.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"23:16\",\n",
            "            \"end_time\": \"24:16\",\n",
            "            \"reason\": \"Thomas Kurian introduces Gemini 1.5 Pro and speaks about its enhanced capabilities.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_12.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_12.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_12_transcript.json\",\n",
            "            \"transcription\": \" That's why we've already started deploying at scale Google services on arm based instances, including Spanner, big cory, GKE, Google Earth Engine and the YouTube Ads platform. Now let's dive into Vertex AI, our fast growing enterprise AI platform. In our Vertex AI model Garden, you can access over 130 models, including the latest versions of Gemini, part of.  models like Cloud from Anthropic and popular open models including Lama, Gemma and Mistral in a variety of configurations. You choose the best model for your use case, budget and performance needs and switch between models as you need to. To get today, we're taking Gemini 1.5 Pro into public preview.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"speaker introduction\",\n",
            "            \"start_time\": \"31:18\",\n",
            "            \"end_time\": \"31:34\",\n",
            "            \"reason\": \"The CEO of Mercedes-Benz, Ola K\\u00e4llenius, is introduced on stage.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_13.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_13.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_13_transcript.json\",\n",
            "            \"transcription\": \" At Mercedes-Benz.\"\n",
            "        },\n",
            "        {\n",
            "            \"type\": \"product announcement\",\n",
            "            \"start_time\": \"31:34\",\n",
            "            \"end_time\": \"32:50\",\n",
            "            \"reason\": \"Ola K\\u00e4llenius speaks about the partnership of Mercedes-Benz with Google and how the company is using Google Cloud AI to equip its cars.\",\n",
            "            \"split_video_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_14.mp4\",\n",
            "            \"split_audio_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_14.wav\",\n",
            "            \"split_transcription_uri\": \"gs://editorial-solaris-demo-output/splits/Google Cloud Next 24 - Keynote/Google Cloud Next 24 - Keynote_split_14_transcript.json\",\n",
            "            \"transcription\": \" We want to offer our customers an exceptional digital experience, that's why we're equipping our cars with high-end computers. Each car should only get better over time, just like a good wine, and with the power of Google Cloud and AI, we will make the user experience even more personalized. Our partnership across Google helps us build more intuitive and customized experiences. Last year we announced our partnership with Google.  and today more than 3 million customers are using google places in their mercedes cars and we are applying google cloud AI across a number of other use cases ranging from a smart sales assistant, improving customer service in our call centers and optimizing our marketing. the sales assistant for example helps customers to seamlessly interact with mercedes when booking a test drive or navigating through Mercedes's offerings to...  find their next favorite vehicle, and now we're exploring further opportunities to work with Google Cloud AI, such as next level navigation features. In addition, we're partnering on one of the most exciting technology topics in our industry.\"\n",
            "        }\n",
            "    ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## News Articles"
      ],
      "metadata": {
        "id": "LnCPjkWdTtd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load all news articles\n",
        "articles_list = []\n",
        "for file in file_list:\n",
        "  if file['type'] == 'text/html':\n",
        "    articles_list.append(file)\n",
        "articles_list"
      ],
      "metadata": {
        "id": "cNrc-CozTxQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary metadata ðŸ§™"
      ],
      "metadata": {
        "id": "P1gMl0HdUx7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_article_metadata(prompt : list, model :str = MODEL) -> str:\n",
        "  vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "  model = GenerativeModel(\n",
        "    model,\n",
        "  )\n",
        "\n",
        "  generation_config = {\n",
        "      \"max_output_tokens\": 8192,\n",
        "      \"temperature\": 0.6,\n",
        "      \"top_p\": 0.95,\n",
        "  }\n",
        "\n",
        "  safety_settings = {\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "      generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,\n",
        "  }\n",
        "\n",
        "  responses = model.generate_content(\n",
        "      prompt,\n",
        "      generation_config=generation_config,\n",
        "      safety_settings=safety_settings,\n",
        "      stream=False,\n",
        "  )\n",
        "\n",
        "  return responses.text\n",
        "\n"
      ],
      "metadata": {
        "id": "6X2v1HhXT-xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a summary of the articles\n",
        "def generate_metadata_articles_overview(blob_uri: str) -> str:\n",
        "  article = Part.from_uri(\n",
        "      mime_type=\"text/html\",\n",
        "      uri=blob_uri)\n",
        "\n",
        "  text1 = \"\"\"SYSTEM:```You are a skilled news analysis expert. You have a deep understanding of media. Your task is to analyze the provided video and extract key information.```\n",
        "  INSTRUCTION: ```\n",
        "   You are a content creation assistant.\n",
        "   You provide information for journalists to get the information they need from articles.\n",
        "\n",
        "   Summary the following [news_content] into three bullet points.\n",
        "\n",
        "   [news_content]:\n",
        "  ```\n",
        "  OUTPUT:```=\n",
        "  JSON\n",
        "  {\n",
        "    \"short_summary\": \"[One paragraph summary of the content]\",\n",
        "    \"long_summary\": \"[two-three paragraph summary of the content]\",\n",
        "    \"subject_topics\" :\n",
        "      [     { \"topic\": \"[topic]\"}, { \"topic\": \"[topic]\"} ]\n",
        "  }```\n",
        "  \"\"\"\n",
        "\n",
        "  result_text = generate_article_metadata(prompt=[text1, article], model = MODEL )\n",
        "  # result_text = generate(prompt=[video1, text1], model = MODEL)\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "SHkyeyUSVE5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a summary of the articles\n",
        "def generate_metadata_articles_headline(blob_uri: str) -> str:\n",
        "  article = Part.from_uri(\n",
        "      mime_type=\"text/html\",\n",
        "      uri=blob_uri)\n",
        "\n",
        "  text1 = \"\"\"SYSTEM:```You are a skilled news analysis expert. You have a deep understanding of media. Your task is to analyze the provided video and extract key information.```\n",
        "  INSTRUCTION: ```\n",
        "  You are a content creation assistant.\n",
        "  You provide information for journalists to get the information they need from articles.\n",
        "  generate a relevant headline for [news_content].\n",
        "  [news_content]:\n",
        "  ```\n",
        "  OUTPUT:```=\n",
        "\n",
        "    \"headline\": \"[a title for the news article]\"\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "  result_text = generate_article_metadata(prompt=[text1, article], model = MODEL )\n",
        "  # result_text = generate(prompt=[video1, text1], model = MODEL)\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "tt9FXQBaeAH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Going through all articles and create their metadata"
      ],
      "metadata": {
        "id": "B4Y8uujndpdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_row_article_summary(article):\n",
        "    \"\"\"Processes a single row from the articles array.\"\"\"\n",
        "    try:\n",
        "        blob_uri = f\"gs://{INPUT_BUCKET}/{article['file_name']}\"\n",
        "        result = generate_metadata_articles_overview(blob_uri=blob_uri)\n",
        "        response_text = re.sub(r\"json|```\", \"\", result)\n",
        "        summary = json.loads(response_text)\n",
        "        article['summary'] = summary\n",
        "\n",
        "        # headlines\n",
        "        result = generate_metadata_articles_headline(blob_uri=blob_uri)\n",
        "        response_text = re.sub(r\"json|```\", \"\", result)\n",
        "        headline = json.loads(response_text)\n",
        "        article['headline'] = headline\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing article {article['file_name']}: {e}\")\n",
        "\n",
        "\n",
        "# Number of worker threads (adjust based on your system and workload)\n",
        "num_workers = 4  # Or use os.cpu_count() for a reasonable default\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_row_article_summary, article) for article in articles_list]\n",
        "\n",
        "    # waiting for threads to complete\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('News Articles Summary Metadata Generated')"
      ],
      "metadata": {
        "id": "tOnZyS2Qa8Oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Analysis"
      ],
      "metadata": {
        "id": "GQcsFKVCy7WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a summary of the articles\n",
        "def generate_metadata_articles_sentiment(blob_uri: str) -> str:\n",
        "  article = Part.from_uri(\n",
        "      mime_type=\"text/html\",\n",
        "      uri=blob_uri)\n",
        "\n",
        "  text1 = \"\"\"SYSTEM:```You are a skilled news analysis expert. You have a deep understanding of media. Your task is to analyze the provided video and extract key information.```\n",
        "  INSTRUCTION: ```\n",
        "  You are a content creation assistant.\n",
        "  You provide information for journalists.\n",
        "  Do sentiment analysis on the different topics in the article [news_content].\n",
        "\n",
        "  [news_content]:\n",
        "  ```\n",
        "  OUTPUT:```=\n",
        "  JSON\n",
        "  {\n",
        "    \"sentiment_analysis\" :\n",
        "      [\n",
        "        { \"topic\": \"[topic]\",\n",
        "          \"score\": \"[Rating how negative the topic is perceived or how positive. score should be -1 and 1]\"\n",
        "          \"magnitude\": \"[Rating the magnitude of the sentiment towards this topic. magnitude should be 0 and 1]\"\n",
        "        },\n",
        "        { \"topic\": \"[topic]\",\n",
        "          \"score\": \"[Rating how negative the topic is perceived or how positive. score should be -1 and 1]\"\n",
        "          \"magnitude\": \"[Rating the magnitude of the sentiment towards this topic. magnitude should be 0 and 1]\"\n",
        "        }\n",
        "  }\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "  result_text = generate_article_metadata(prompt=[text1, article], model = MODEL )\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "-sIKmkZQzBFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_row_article_summary(article):\n",
        "    \"\"\"Processes a single row from the articles array.\"\"\"\n",
        "    try:\n",
        "        blob_uri = f\"gs://{INPUT_BUCKET}/{article['file_name']}\"\n",
        "        result = generate_metadata_articles_sentiment(blob_uri=blob_uri)\n",
        "        response_text = re.sub(r\"json|```\", \"\", result)\n",
        "        summary = json.loads(response_text)\n",
        "        article['sentiment_analysis'] = summary['sentiment_analysis']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing article {article['file_name']}: {e}\")\n",
        "\n",
        "\n",
        "# Number of worker threads (adjust based on your system and workload)\n",
        "num_workers = 4  # Or use os.cpu_count() for a reasonable default\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_row_article_summary, article) for article in articles_list]\n",
        "\n",
        "    # waiting for threads to complete\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('News Articles Summary Metadata Generated')"
      ],
      "metadata": {
        "id": "l9zNqFAb0Fix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for article in articles_list[:1]:\n",
        "  print(article['sentiment_analysis'])"
      ],
      "metadata": {
        "id": "jGr-gII10cNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Generation\n",
        "In this section we mimic how generative ai could be applied to create the header images of the articles that we are processing"
      ],
      "metadata": {
        "id": "jn46idhD0AGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we will ask Gemini to create a suitable prompt for the given news article"
      ],
      "metadata": {
        "id": "SksugtW6-bK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#create a summary of the articles\n",
        "def generate_metadata_articles_image_prompt(blob_uri: str) -> str:\n",
        "  article = Part.from_uri(\n",
        "      mime_type=\"text/html\",\n",
        "      uri=blob_uri)\n",
        "\n",
        "  text1 = \"\"\"SYSTEM:```You are a skilled news analysis expert. You have a deep understanding of media.```\n",
        "  INSTRUCTION: ```\n",
        "  You are a content creation assistant.\n",
        "  You understand the content that is in news_content and provide a suitable prompt for image generation.\n",
        "  Generate up to two prompts\n",
        "\n",
        "  [news_content]:\n",
        "  ```\n",
        "  OUTPUT:```=\n",
        "  JSON\n",
        "  {\n",
        "    \"image_generation_prompts\" :\n",
        "      [\n",
        "        {\n",
        "          \"prompt_number\" : \"[Number of the prompt suggest. Goes from 0 onwards]\",\n",
        "          \"prompt\": \"[instructions for the image generation model]\",\n",
        "          \"reasoning\": \"Description of the decision behind this image generation prompt\"\n",
        "          \"style\": \"[Best artistic style for this prompt]\"\n",
        "        },\n",
        "        {\n",
        "          \"prompt_number\" : \"[Number of the prompt suggest. Goes from 0 onwards]\",\n",
        "          \"prompt\": \"[instructions for the image generation model]\",\n",
        "          \"reasoning\": \"Description of the decision behind this image generation prompt\"\n",
        "          \"style\": \"[Best artistic style for this prompt]\"\n",
        "        }\n",
        "  }\n",
        "  ```\n",
        "  \"\"\"\n",
        "\n",
        "  result_text = generate_article_metadata(prompt=[text1, article], model = MODEL )\n",
        "\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "C9TcqMO2-im7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_row_article_image_prompt(article):\n",
        "    \"\"\"Processes a single row from the articles array.\"\"\"\n",
        "    try:\n",
        "        blob_uri = f\"gs://{INPUT_BUCKET}/{article['file_name']}\"\n",
        "        result = generate_metadata_articles_image_prompt(blob_uri=blob_uri)\n",
        "        response_text = re.sub(r\"json|```\", \"\", result)\n",
        "        summary = json.loads(response_text)\n",
        "        article['image_generation_prompts'] = summary['image_generation_prompts']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing article {article['file_name']}: {e}\")\n",
        "\n",
        "\n",
        "# Number of worker threads (adjust based on your system and workload)\n",
        "num_workers = 4  # Or use os.cpu_count() for a reasonable default\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # Submit tasks to the executor\n",
        "    futures = [executor.submit(process_row_article_image_prompt, article) for article in articles_list]\n",
        "\n",
        "    # waiting for threads to complete\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('News Articles Image generation promtp Generated')"
      ],
      "metadata": {
        "id": "0mIH33zI_HqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Show the example of one article and it's respective prompts"
      ],
      "metadata": {
        "id": "lbu16f0__74f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "articles_list[0]['image_generation_prompts']"
      ],
      "metadata": {
        "id": "Wj6i_MC0_z3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate one image per prompt"
      ],
      "metadata": {
        "id": "y8R7sued_c7g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#image generation imports\n",
        "from vertexai.preview.vision_models import ImageGenerationModel\n",
        "# start the model\n",
        "\n",
        "def generate_image(prompt : str, style : str):\n",
        "  generation_model = ImageGenerationModel.from_pretrained(\"imagen-3.0-generate-001\")\n",
        "\n",
        "  final_prompt = f\"\"\"{prompt}. The style should be: {style}\"\"\"\n",
        "\n",
        "  response = generation_model.generate_images(\n",
        "      prompt=final_prompt,\n",
        "      number_of_images=1,\n",
        "  )\n",
        "\n",
        "  return response"
      ],
      "metadata": {
        "id": "s-hF3_ag9scm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for article in articles_list:\n",
        "  article['name'] = article['file_name'].split('.')[0]\n"
      ],
      "metadata": {
        "id": "phZpvx0POMkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_row_prompt_image(prompt, article):\n",
        "    \"\"\"Processes a single row from the articles array.\"\"\"\n",
        "    try:\n",
        "        result = generate_image(prompt=prompt['prompt'], style=prompt['style'])\n",
        "        if(result.images):\n",
        "          file_name = f\"articles/images/{article['name']}_{prompt['prompt_number']}.jpeg\"\n",
        "          prompt['image_path'] = file_name\n",
        "          result.images[0].save(location=file_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating image prompt {prompt['prompt']}: {e}\")\n",
        "\n",
        "#create thumbnails folder if it doesn't exist\n",
        "splits_folder = f\"articles/images/\"\n",
        "if not os.path.exists(splits_folder):\n",
        "    os.makedirs(splits_folder)\n",
        "\n",
        "# Number of worker threads (adjust based on your system and workload)\n",
        "num_workers = 4  # Or use os.cpu_count() for a reasonable default\n",
        "\n",
        "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
        "    # Submit tasks to the executor\n",
        "    for article in articles_list:\n",
        "      futures = [executor.submit(process_row_prompt_image, prompt, article) for prompt in article['image_generation_prompts']]\n",
        "\n",
        "    # waiting for threads to complete\n",
        "    for _ in tqdm.tqdm(as_completed(futures), total=len(futures)):\n",
        "        pass  # No need to process individual results here\n",
        "\n",
        "print('News Articles Images Generated')"
      ],
      "metadata": {
        "id": "O9Ca0SZtDqWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for article in articles_list:\n",
        "  for prompt in article['image_generation_prompts']:\n",
        "    file_name = f\"articles/images/{article['name']}_{prompt['prompt_number']}.jpeg\"\n",
        "    prompt['image_path'] = file_name"
      ],
      "metadata": {
        "id": "ZqOfc4NaLNDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Move all files in content/splits to a gcs bucket using gsutils\n",
        "!gsutil -m cp -r articles/images {OUTPUT_BUCKET}/articles/images"
      ],
      "metadata": {
        "id": "DNsJ0atWKdTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For each of the prompts generate the image and add it in the correct json object\n",
        "# for article in articles_list:\n",
        "#   for prompt in article['image_generation_prompts']:\n",
        "#     result = generate_image(prompt=prompt['prompt'], style=prompt['style'])\n",
        "#     prompt['image'] = result.images[0]\n",
        "\n"
      ],
      "metadata": {
        "id": "hDA2jYoXAZEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o-GxTrNNGdEa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qpZzhB8kGdB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# article = articles_list[0]\n",
        "# file_name = f\"articles/images/{article['name']}_0.jpeg\"\n",
        "# articles_list[0]['image_generation_prompts'][0]['image'].save(location=file_name)"
      ],
      "metadata": {
        "id": "rSnbw8UB-OMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Source system metadata"
      ],
      "metadata": {
        "id": "fKnrHYQfy8-t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part shouldn't be required in a production environment but we are mimicking data that would come from the source systems."
      ],
      "metadata": {
        "id": "ZcZrcQs3uCx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create metadata that could be inferred by the source systems\n",
        "# this is completely custom to your naming conventions. In this case we assumed that file names are multiple words separated by hyphens\n",
        "for article in tqdm.tqdm(articles_list, desc=\"Creating additional articles metadata\"):\n",
        "  name = article['file_name'].split('.')[:-1]\n",
        "  name = name[0].split('-')\n",
        "  name = ' '.join(name).title()\n",
        "  article['name'] = name\n",
        "  article['content'] = read_string_from_gcs(bucket_name= INPUT_BUCKET, file_name= article['file_name'])"
      ],
      "metadata": {
        "id": "d4krnTDMsO80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Storing Metadata ðŸ’¾"
      ],
      "metadata": {
        "id": "iOW2QOpEpRR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# starting the firebase db\n",
        "# Initialize Firebase Admin SDK\n",
        "firebase_admin.initialize_app()\n"
      ],
      "metadata": {
        "id": "DaTDafZRiwnb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a reference to the Firestore database\n",
        "db = firestore.client()"
      ],
      "metadata": {
        "id": "kt4YJMCNuydy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for audio in tqdm.tqdm(audio_list, desc=\"Storing audio metada\"):\n",
        "  doc_ref = db.collection(AUDIO_COLLECTION).document(audio['name'])\n",
        "  doc_ref.set(audio)"
      ],
      "metadata": {
        "id": "7BXgnwoBcS-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image in tqdm.tqdm(image_list, desc=\"Storing images metada\"):\n",
        "  doc_ref = db.collection(IMAGE_COLLECTION).document(image['name'])\n",
        "  doc_ref.set(image)"
      ],
      "metadata": {
        "id": "RH82ANgYTQbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Storing the video information\n",
        "for video in tqdm.tqdm(video_list, desc=\"Storing videos metada\"):\n",
        "  if 'sections' in video:\n",
        "    video_name = video['file_name'].split('.')[0]\n",
        "    doc_ref = db.collection(VIDEO_COLLECTION).document(video_name)\n",
        "\n",
        "    # Insert the JSON object into the document\n",
        "    doc_ref.set(video)"
      ],
      "metadata": {
        "id": "_-MmhGGBkS51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59736988-d3f2-49a0-8509-a18d40cf7a3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Storing videos metada: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  7.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for article in tqdm.tqdm(articles_list, desc=\"Storing articles metada\"):\n",
        "  doc_ref = db.collection(ARTICLE_COLLECTION).document(article['file_name'])\n",
        "  doc_ref.set(article)"
      ],
      "metadata": {
        "id": "bQYqLd_fb8DS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil -m mv -r gs://{PIPELINE_INPUT_BUCKET}/* gs://{INPUT_BUCKET}/"
      ],
      "metadata": {
        "id": "7lCSmsg_tl05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings\n"
      ],
      "metadata": {
        "id": "mnNysGzEBSJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Embeddings Creation"
      ],
      "metadata": {
        "id": "b-IHagebNFP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vertexai.init(project=PROJECT_ID, location=LOCATION)"
      ],
      "metadata": {
        "id": "JLf2mKqMBaVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n"
      ],
      "metadata": {
        "id": "FVs7oY62Bb7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings_wrapper(texts):\n",
        "    BATCH_SIZE = 5\n",
        "    embs = []\n",
        "    for i in tqdm.tqdm(range(0, len(texts), BATCH_SIZE)):\n",
        "        time.sleep(1)  # to avoid the quota error\n",
        "        result = model.get_embeddings(texts[i : i + BATCH_SIZE])\n",
        "        embs = embs + [e.values for e in result]\n",
        "    return embs"
      ],
      "metadata": {
        "id": "WcDDqxAfBWC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def write_embeddings_to_jsonl(embeddings, filename):\n",
        "  \"\"\"Writes an array of embeddings with IDs to a JSONL file.\n",
        "\n",
        "  Args:\n",
        "    embeddings: An array of dictionaries, where each dictionary has 'id' and 'embedding' keys.\n",
        "    filename: The name of the JSONL file to write to.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(filename, 'w') as f:\n",
        "    for embedding in embeddings:\n",
        "      json.dump(embedding, f)  # Write each embedding as a JSON object\n",
        "      f.write('\\n')  # Add a newline character after each object\n"
      ],
      "metadata": {
        "id": "Z7wpBVgHB3as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Article embeddings"
      ],
      "metadata": {
        "id": "P15LDAokOwt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the firestore entries into a pandas dataframe."
      ],
      "metadata": {
        "id": "LuA9tpqmN-kd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all documents from the collection\n",
        "docs = db.collection(ARTICLE_COLLECTION).stream()\n",
        "\n",
        "# Convert Firestore documents to a list of dictionaries\n",
        "data = []\n",
        "for doc in docs:\n",
        "    data.append(doc.to_dict())\n",
        "\n",
        "df_articles = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "VC1uKZ-kC1sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DGAtbfWiFEjE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c6f120a-557f-47e5-fc5a-e4a29a49c636"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document data: {'file_name': 'Make up series YT.mp4', 'type': 'video/mp4', 'sections': [{'start_time': '0:00', 'type': 'introduction', 'split_transcription_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_0_transcript.json', 'reason': \"The video starts with a clapperboard introducing the YouTube series 'Our Makeup' and then transitions to a shot of the two hosts.\", 'split_video_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_0.mp4', 'split_audio_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_0.wav', 'end_time': '0:03'}, {'split_video_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_1.mp4', 'type': 'self introduction', 'reason': 'Both the hosts introduce themselves and their background.', 'end_time': '0:29', 'split_transcription_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_1_transcript.json', 'transcription': \" Nina, introduce yourself. Hello, I'm Nina Fitzgerald. I'm the creative director behind the Our makeup series, I'm a proud avergal straight islander woman, born and raised on Larakia country in the northern territory. I'm really excited to flip the script today and chat to my really good friend and the host of the our makeup series, makeup artist Katie Karl Taylor.\", 'split_audio_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_1.wav', 'start_time': '0:03'}, {'split_audio_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_2.wav', 'split_video_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_2.mp4', 'end_time': '1:06', 'split_transcription_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_2_transcript.json', 'type': \"Kaydee's Background\", 'reason': 'Kaydee talks about her background and her connection to her Aboriginal roots and the places where her family comes from.', 'start_time': '0:29', 'transcription': \" thank you for having me for the flip the script moment. so tell us about yourself and your makeup. i'm a proud original multi woman. um, i live and create here in nam Melbourne. i originally grew up in miajin in Brisbane. and that's where all my mob is from back in Brisbane and farnorth Queensland. i've been around in the fashion and beauty industry as a respected makeup artist. i work across the whole industry. um, i don't like to put myself into one specific.  category or box, so I just want to go back to where your family is from, you said you grew up in Manjan and your family is also from the Cape York.\"}, {'reason': 'Kaydee shares her early interest in makeup, and how she got into makeup artistry.', 'transcription': \" paninsula: yeah, so I'm Luma lama, Wakawaka, beerig, which is from Cape York as well as then, like I have then connections to Wakawaka, which is Sherberg mission, yeah, um, that's where my grandmother's from, um, and her fatherers, I was raised around my grandmother heap, so I remember my childhood always kind of being with her and with all my cousins, and so then you did you take up makeup artistry post high school, yeah, because it was...  because when mum saw me kind of pick up a brush and i've been wearing like press ons playing with my hair color and i was getting like ripped at school by the principals cuz they were just like she's coming with press on.\", 'type': 'Early interest in Makeup', 'split_video_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_3.mp4', 'split_transcription_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_3_transcript.json', 'start_time': '1:06', 'end_time': '1:48', 'split_audio_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_3.wav'}, {'split_transcription_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_4_transcript.json', 'start_time': '1:48', 'type': 'Lack of Representation', 'reason': 'Kaydee explains the lack of representation she experienced in the beauty industry during her diploma and how it impacted her career.', 'split_video_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_4.mp4', 'split_audio_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_4.wav', 'end_time': '2:29', 'transcription': \" nails again, so when you were doing your makeup diploma and noticing the lack of representation, how did that inspire your career and what you've gone up to do now? it came down to even like the small things of like the cosmetic range that we had available to us was completely lacking in shade diversity which was crazy. we also would have only 2 hours of the whole year that we had allocated to darker complexion and like 2 hours.  is only to darker complexion and um people have colored the hair textures, two hours only out of a whole year of worth of studies like over the years, i've definitely challenged the industry on the...\"}, {'start_time': '2:29', 'type': 'The New Series', 'end_time': '3:18', 'transcription': \" you've done it unapologetically, like you said before, that's the other thing, you're not trying to hide, I'm not trying to hide, but I'm also trying to bring awareness. I'm fast forward to now on this kind of idea of representation in the beauty industry, but also just creative industries more broadly. um, it's so exciting that we're making this series with yourself, Becca Hatch, Cindy Rostan and Sherry Watson. How does it feel for you being a part of this? It's really special and I, I know...  all these all these three sisters that we're that we've been with, so it's just nice to kind of put a spotlight on them as well, um, and talk about it through a makeup lens and include me in that aspect of it all, it's really nice as someone who's been in the industry for a long time and at the start was...\", 'split_video_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_5.mp4', 'split_transcription_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_5_transcript.json', 'reason': 'Nina and Kaydee discuss the new series, highlighting the importance of representation in the beauty and creative industries.', 'split_audio_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_5.wav'}, {'transcription': \" was very alone versus like now and there's like mob in every aspect whether it's in front of a camera or behind the camera, it's like just seeing that kind of empowerment. definitely something new in terms of you've just ticked a little box where like have you hosted a makeup series now the now done i was like will i do it again?\", 'start_time': '3:18', 'type': \"Kaydee's Feelings\", 'reason': 'Kaydee shares her feelings about being part of the project and the progress made in representation within the industry.', 'end_time': '3:38', 'split_video_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_6.mp4', 'split_transcription_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_6_transcript.json', 'split_audio_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_6.wav'}, {'split_audio_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_7.wav', 'split_transcription_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_7_transcript.json', 'split_video_uri': 'gs://editorial-solaris-demo-output/splits/Make up series YT/Make up series YT_split_7.mp4', 'reason': \"Nina thanks Kaydee for the interview and expresses excitement for the release of 'Our Makeup' on YouTube.\", 'transcription': \" Maybe with some media training, yeah, you don't need media training. Thank you so much for chatting with me today. Um, having me, can't wait to see the full series of our makeup, so excited, only on YouTube.\", 'end_time': '3:54', 'type': 'Conclusion', 'start_time': '3:38'}], 'size': 58222286, 'thumbnails': [{'reason': 'Kaydee is shown holding a clapperboard, indicating the start of the interview. ', 'time': '00:00'}, {'time': '00:21', 'reason': 'Kaydee and Nina sitting on the couch, engaged in a conversation about the topic.'}, {'time': '04:03', 'reason': \"Kaydee smiles broadly and looks directly at the camera, conveying her excitement for the series and aligning with the audio of her saying 'excited'.\"}], 'name': 'Make up series YT', 'updated': DatetimeWithNanoseconds(2024, 9, 23, 21, 57, 21, 124000, tzinfo=datetime.timezone.utc), 'summary': {'subject_topics': [{'topic': 'Makeup'}, {'topic': 'Beauty Industry'}, {'topic': 'Diversity'}, {'topic': 'Representation'}, {'topic': 'Indigenous Culture'}, {'topic': 'YouTube Series'}], 'long_summary': \"This trailer introduces a new YouTube makeup series, 'Our Makeup.' Nina Fitzgerald, the creative director, describes herself as a proud Aboriginal and Torres Strait Islander woman and shares her excitement about the series, which aims to flip the script on traditional beauty standards. Kaydee Kyle-Taylor, the host and a respected makeup artist, is introduced as a proud Aboriginal and Multi woman.  She explains her diverse heritage, connecting to the Lama Lama, Wakawaka, and Birri Gaba peoples of Australia and the NgÄti Porou and NgÄti Kahungunu peoples of New Zealand.  Kaydee shares her passion for makeup, recalling how she was drawn to it from a young age, even facing criticism for expressing her creativity through makeup and hairstyles in school. The conversation then shifts to the lack of representation in the beauty industry. Kaydee recounts her experience during her makeup diploma where she observed a significant lack of shade diversity and limited curriculum time dedicated to darker complexions and textured hair.  She expresses her determination to challenge these norms and raise awareness about the need for greater inclusivity.  The trailer concludes with a message highlighting the importance of representation in the beauty industry and the series' commitment to showcasing diverse talent, both in front of and behind the camera.\", 'short_summary': \"This video is a trailer for a YouTube makeup series called 'Our Makeup.'  The creative director of the series, Nina Fitzgerald, introduces the host, makeup artist Kaydee Kyle-Taylor, who is a proud Aboriginal and Multi woman. Kaydee discusses her journey as a makeup artist, highlighting the lack of representation she experienced during her studies and her efforts to challenge the industry's diversity standards.\"}, 'created': DatetimeWithNanoseconds(2024, 9, 23, 21, 57, 21, 124000, tzinfo=datetime.timezone.utc)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a column that will have the content that will be embedded"
      ],
      "metadata": {
        "id": "iR-FPZlmPbZ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create embedding content\n",
        "def create_embedding_content_article(row):\n",
        "  json_obj = {\n",
        "      'content' : row['content'].decode(encoding='utf-8'),\n",
        "      'summary': row['summary']['short_summary'],\n",
        "      'subject_topics' :row['summary']['subject_topics'] }\n",
        "  return json.dumps(json_obj)\n",
        "\n",
        "# create the column that will be embedded\n",
        "df_articles['embedding_content'] = df_articles.apply(create_embedding_content_article, axis=1)\n",
        "# Create and id column\n",
        "df_articles['id'] = 'article:'+df_articles['file_name']"
      ],
      "metadata": {
        "id": "l-PNHLVGPact"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the embeddings, assign and id column to the dataframe"
      ],
      "metadata": {
        "id": "5EbWmQANPVor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_articles = df_articles.assign(embedding=get_embeddings_wrapper(list(df_articles.embedding_content)))"
      ],
      "metadata": {
        "id": "GZdS50aWPUOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b127ba-aaad-4721-f7d0-ba368747910e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.53s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image embeddings"
      ],
      "metadata": {
        "id": "yQKMjQR5O507"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the firestore entries into a pandas dataframe."
      ],
      "metadata": {
        "id": "UOxIfemdPfna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all documents from the collection\n",
        "docs = db.collection(IMAGE_COLLECTION).stream()\n",
        "\n",
        "# Convert Firestore documents to a list of dictionaries\n",
        "data = []\n",
        "for doc in docs:\n",
        "    data.append(doc.to_dict())\n",
        "\n",
        "df_images = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "Y3ZtFI6ZPfna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_images = df_images.dropna(subset=['metadata'])"
      ],
      "metadata": {
        "id": "0vBApsj-i-Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create embedding content\n",
        "def create_embedding_content_image(row):\n",
        "  json_obj = {\n",
        "      'name' : row['name'],\n",
        "      'description' : row['metadata']['description'],\n",
        "      'location' :  row['metadata']['location'] if 'location' in row['metadata'] else None,\n",
        "      'photo_type' : row['metadata']['photo_type'],\n",
        "      'description': row['metadata']['description'],\n",
        "      'subject_topics' :row['metadata']['subject_topics'] }\n",
        "  return json.dumps(json_obj)\n",
        "\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_images['embedding_content'] = df_images.apply(create_embedding_content_image, axis=1)\n",
        "# Create and id column\n",
        "df_images['id'] = 'image:'+df_images['name']"
      ],
      "metadata": {
        "id": "HynMCIXYWJZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the embeddings, assign and id column to the dataframe"
      ],
      "metadata": {
        "id": "YZetJ3xFaAgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_images = df_images.assign(embedding=get_embeddings_wrapper(list(df_images.embedding_content)))"
      ],
      "metadata": {
        "id": "r0Aj2rgOaAgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3715b3b-843d-4720-8d14-230192957a98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.13s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Audio embeddings"
      ],
      "metadata": {
        "id": "Mt5j0IcjZhVu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the firestore entries into a pandas dataframe."
      ],
      "metadata": {
        "id": "qw2WmeZ7ZhVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all documents from the collection\n",
        "docs = db.collection(AUDIO_COLLECTION).stream()\n",
        "\n",
        "# Convert Firestore documents to a list of dictionaries\n",
        "data = []\n",
        "for doc in docs:\n",
        "    data.append(doc.to_dict())\n",
        "\n",
        "df_audio = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "xyZ4mk5YZhVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO - Load transcription as well\n",
        "# Function to create embedding content\n",
        "def create_embedding_content_audio(row):\n",
        "  json_obj = {\n",
        "      'name' : row['name'],\n",
        "      'show_name' :  row['metadata']['show_name'],\n",
        "      'summary': row['metadata']['short_summary'],\n",
        "      'subject_topics' :row['metadata']['subject_topics'] }\n",
        "  return json.dumps(json_obj)\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_audio['embedding_content'] = df_audio.apply(create_embedding_content_audio, axis=1)\n",
        "# Create and id column\n",
        "df_audio['id'] = 'audio:'+df_audio['name']"
      ],
      "metadata": {
        "id": "72JFS2qxZhVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_audio = df_audio.assign(embedding=get_embeddings_wrapper(list(df_audio.embedding_content)))"
      ],
      "metadata": {
        "id": "ITruJSzVa3bs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfefa1fb-e786-47c8-e788-66ed70f833c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:01<00:00,  1.18s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Video embeddings"
      ],
      "metadata": {
        "id": "2BsCgYBwO3IN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the firestore entries into a pandas dataframe."
      ],
      "metadata": {
        "id": "3AjcmBIwO99h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all documents from the collection\n",
        "docs = db.collection(VIDEO_COLLECTION).stream()\n",
        "\n",
        "# Convert Firestore documents to a list of dictionaries\n",
        "data = []\n",
        "for doc in docs:\n",
        "    data.append(doc.to_dict())\n",
        "\n",
        "df_videos = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "2CU_O_DUO99i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create embedding content for video object\n",
        "def create_embedding_content_video(row):\n",
        "  json_obj = {\n",
        "      'name' : row['name'],\n",
        "      'summary': row['summary']['short_summary'],\n",
        "      'long_summary': row['summary']['long_summary'],\n",
        "      'subject_topics' :row['summary']['subject_topics'] }\n",
        "  return json.dumps(json_obj)\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_videos['embedding_content'] = df_videos.apply(create_embedding_content_video, axis=1)\n",
        "# Create and id column\n",
        "df_videos['id'] = 'video:'+df_videos['name']"
      ],
      "metadata": {
        "id": "r9LnF0y9VusS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_videos = df_videos.assign(embedding=get_embeddings_wrapper(list(df_videos.embedding_content)))"
      ],
      "metadata": {
        "id": "aFP-3TmDezR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "808f1c66-afab-40ac-9208-f4e6ae776146"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:02<00:00,  1.21s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For videos there is an additional layer we can add - the key moments. By embeddings this content, you'll be able to pinpoint which actual part of the video is relevant to the search that the user does."
      ],
      "metadata": {
        "id": "nypPm6YFcHkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all documents from the collection\n",
        "docs = db.collection(VIDEO_COLLECTION).stream()\n",
        "\n",
        "# load the previous data into a new dataframe by flatening it to the key segments / sections level\n",
        "segments = []\n",
        "for doc in docs:\n",
        "  video = doc.to_dict()\n",
        "  if 'sections' in video:\n",
        "    section_num =0\n",
        "    for section in video['sections']:\n",
        "      section['id'] = f\"video:{section_num}:{video['name']}\"\n",
        "      segments.append(section)\n",
        "      section_num +=1\n",
        "\n",
        "df_segments = pd.DataFrame(segments)"
      ],
      "metadata": {
        "id": "wEeHmNptcU0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create embedding content for video object\n",
        "def create_embedding_content_segment(row):\n",
        "  json_obj = {\n",
        "      'section' : row['reason'],\n",
        "      'summary' : row['type'],\n",
        "      'transcription': row['transcription']\n",
        "      }\n",
        "  return json.dumps(json_obj)\n",
        "\n",
        "# Apply the function to create the new column\n",
        "df_segments['embedding_content'] = df_segments.apply(create_embedding_content_segment, axis=1)"
      ],
      "metadata": {
        "id": "MkKh5ncyec5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_segments = df_segments.assign(embedding=get_embeddings_wrapper(list(df_segments.embedding_content)))"
      ],
      "metadata": {
        "id": "ijFi6f19e9LG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74d7ea4-3cb9-450a-ad9d-053982488519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12/12 [00:13<00:00,  1.11s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merge embeddings"
      ],
      "metadata": {
        "id": "OZO7raNPYdEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_df = pd.concat([df_articles, df_images, df_audio, df_videos, df_segments], ignore_index=True)"
      ],
      "metadata": {
        "id": "lgIcLbyGYfXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save id and embedding as a json file\n",
        "jsonl_string = merged_df[[\"id\", \"embedding\"]].to_json(orient=\"records\", lines=True)\n",
        "with open(\"embeddings.json\", \"w\") as f:\n",
        "    f.write(jsonl_string)"
      ],
      "metadata": {
        "id": "UN9oxjHqC0Fd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " !gsutil cp embeddings.json gs://{EMBEDDINGS_BUCKET}"
      ],
      "metadata": {
        "id": "4kwVZ_C0EsXI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62b8e32b-40a4-4e63-d431-99c13e6e25b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copying file://embeddings.json [Content-Type=application/json]...\n",
            "/ [1 files][908.7 KiB/908.7 KiB]                                                \n",
            "Operation completed over 1 objects/908.7 KiB.                                    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Search\n",
        "\n",
        "Setup vector search index, endpoint and deploy the index."
      ],
      "metadata": {
        "id": "y03EHOBwNMCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# init the aiplatform package\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "VS_ITERATION_SUFFIX = '008'\n",
        "VS_INDEX_NAME = f\"embeddings-{VS_ITERATION_SUFFIX}\"\n",
        "VS_INDEX_ENDPOINT_NAME = f\"embeddings-{VS_ITERATION_SUFFIX}-index-endpoint\"\n",
        "VS_DEPLOYED_INDEX_ID = f\"embeddings_deployed_{VS_ITERATION_SUFFIX}\""
      ],
      "metadata": {
        "id": "rFtGOSR9B0No"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create index\n",
        "my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "    display_name=VS_INDEX_NAME,\n",
        "    contents_delta_uri=f\"gs://{EMBEDDINGS_BUCKET}\",\n",
        "    dimensions=768,\n",
        "    approximate_neighbors_count=20,\n",
        "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
        ")"
      ],
      "metadata": {
        "id": "sXjCcDEtE_9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fcc8777-febf-4df9-a5a2-8b6e547fc83d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:Creating MatchingEngineIndex\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:Create MatchingEngineIndex backing LRO: projects/837701957704/locations/us-central1/indexes/7389874824887140352/operations/786722181949161472\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:MatchingEngineIndex created. Resource name: projects/837701957704/locations/us-central1/indexes/7389874824887140352\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:To use this MatchingEngineIndex in another session:\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index:index = aiplatform.MatchingEngineIndex('projects/837701957704/locations/us-central1/indexes/7389874824887140352')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create IndexEndpoint\n",
        "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "    display_name=VS_INDEX_ENDPOINT_NAME,\n",
        "    public_endpoint_enabled=True,\n",
        ")"
      ],
      "metadata": {
        "id": "Q-QCtZpDFt0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "988cac46-6019-44e3-84e1-93710805c154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Creating MatchingEngineIndexEndpoint\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Create MatchingEngineIndexEndpoint backing LRO: projects/837701957704/locations/us-central1/indexEndpoints/5901100896506347520/operations/4856850345185247232\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:MatchingEngineIndexEndpoint created. Resource name: projects/837701957704/locations/us-central1/indexEndpoints/5901100896506347520\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:To use this MatchingEngineIndexEndpoint in another session:\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:index_endpoint = aiplatform.MatchingEngineIndexEndpoint('projects/837701957704/locations/us-central1/indexEndpoints/5901100896506347520')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy the Index to the Index Endpoint\n",
        "my_index_endpoint.deploy_index(index=my_index, deployed_index_id=VS_DEPLOYED_INDEX_ID)\n"
      ],
      "metadata": {
        "id": "hS0eRE78F6iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a0f9f66-d222-42bd-cf60-d60a730e68cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Deploying index MatchingEngineIndexEndpoint index_endpoint: projects/837701957704/locations/us-central1/indexEndpoints/5901100896506347520\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:Deploy index MatchingEngineIndexEndpoint index_endpoint backing LRO: projects/837701957704/locations/us-central1/indexEndpoints/5901100896506347520/operations/1407093030619447296\n",
            "INFO:google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint:MatchingEngineIndexEndpoint index_endpoint Deployed index. Resource name: projects/837701957704/locations/us-central1/indexEndpoints/5901100896506347520\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<google.cloud.aiplatform.matching_engine.matching_engine_index_endpoint.MatchingEngineIndexEndpoint object at 0x7ef662f92740> \n",
              "resource name: projects/837701957704/locations/us-central1/indexEndpoints/5901100896506347520"
            ]
          },
          "metadata": {},
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Index"
      ],
      "metadata": {
        "id": "pFNgHBDfGGNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n"
      ],
      "metadata": {
        "id": "cqRK4CFLsJbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ad-hoc\n"
      ],
      "metadata": {
        "id": "r9qA-TQbdR34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all documents from the collection\n",
        "docs = db.collection(AUDIO_COLLECTION).stream()\n",
        "\n",
        "# Convert Firestore documents to a list of dictionaries\n",
        "audio_list = []\n",
        "for doc in docs:\n",
        "  entry = doc.to_dict()\n",
        "  entry['gcs_uri'] = entry['gcs_uri'].replace('gs://editorial-solaris-demo-input/processed/','gs://editorial-solaris-demo-input/')\n",
        "  audio_list.append(entry)\n",
        "\n",
        "for audio in tqdm.tqdm(audio_list, desc=\"Storing audio metada\"):\n",
        "  doc_ref = db.collection(AUDIO_COLLECTION).document(audio['name'])\n",
        "  doc_ref.set(audio)"
      ],
      "metadata": {
        "id": "xdGF4qdjdYei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Others"
      ],
      "metadata": {
        "id": "UbIxrzqydVBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from vertexai.language_models import TextEmbeddingModel\n",
        "\n",
        "def generate_embeddings(text):\n",
        "    model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")\n",
        "    embeddings = model.get_embeddings([text])\n",
        "    embedding = embeddings[0].values\n",
        "    return embedding"
      ],
      "metadata": {
        "id": "xyAUaQ4F6-nQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_embeddings(doc, selected_properties):\n",
        "  \"\"\"\n",
        "  Prepares embeddings for a document with specified properties.\n",
        "\n",
        "  Args:\n",
        "    doc: The document object (dictionary or similar).\n",
        "    selected_properties: A list of property names to include in the embedding.\n",
        "\n",
        "  Returns:\n",
        "    the embedding object\n",
        "  \"\"\"\n",
        "\n",
        "  selected_data = {prop: doc[prop] for prop in selected_properties if prop in doc}\n",
        "  embedding_content = json.dumps(selected_data)\n",
        "  return  generate_embeddings(embedding_content)\n"
      ],
      "metadata": {
        "id": "upttX4MhTEHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all documents from the collection\n",
        "docs = db.collection(ARTICLE_COLLECTION).stream()\n",
        "\n",
        "# Create an empty list to store the articles' embeddings\n",
        "articles_embeddings = []\n",
        "selected_properties = [ 'headline']\n",
        "\n",
        "# Iterate over the documents and append them to the list\n",
        "for doc in tqdm.tqdm(docs, desc=\"Generating embeddings\"):  #\n",
        "  article = doc.to_dict()\n",
        "  id = 'article:'+doc.id\n",
        "  article['content'] = article['content'].decode(encoding='utf-8')\n",
        "\n",
        "  articles_embeddings.append(f\"{{'id':{id},'embedding':{prepare_embeddings(article, selected_properties)}}} \")"
      ],
      "metadata": {
        "id": "IBbRYm1LtLUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "write the file to a gcs bucket"
      ],
      "metadata": {
        "id": "Nr9xQ4fh7ggQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def write_embeddings_to_jsonl(embeddings, filename):\n",
        "  \"\"\"Writes an array of embeddings with IDs to a JSONL file.\n",
        "\n",
        "  Args:\n",
        "    embeddings: An array of dictionaries, where each dictionary has 'id' and 'embedding' keys.\n",
        "    filename: The name of the JSONL file to write to.\n",
        "  \"\"\"\n",
        "\n",
        "  with open(filename, 'w') as f:\n",
        "    for embedding in embeddings:\n",
        "      json.dump(embedding, f)  # Write each embedding as a JSON object\n",
        "      f.write('\\n')  # Add a newline character after each object\n"
      ],
      "metadata": {
        "id": "gXBtB06h2Vch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create thumbnails folder if it doesn't exist\n",
        "splits_folder = f\"embeddings/\"\n",
        "if not os.path.exists(splits_folder):\n",
        "    os.makedirs(splits_folder)\n",
        "\n",
        "write_embeddings_to_jsonl(articles_embeddings, 'embeddings/articles_embeddings.jsonl')"
      ],
      "metadata": {
        "id": "rDj1WSaq77tx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Moving the JSONL files from the colab environment to the designated bucket"
      ],
      "metadata": {
        "id": "eFf377KK-S7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!gsutil cp embeddings/articles_embeddings.jsonl gs://{EMBEDDINGS_BUCKET}"
      ],
      "metadata": {
        "id": "jS9roSV9tgtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Matching Index"
      ],
      "metadata": {
        "id": "ERIgoIrF-gRl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VS_ITERATION_SUFFIX = '003B'\n",
        "VS_INDEX_NAME = f\"embeddings-{VS_ITERATION_SUFFIX}\"\n",
        "VS_INDEX_ENDPOINT_NAME = f\"embeddings-{VS_ITERATION_SUFFIX}-index-endpoint\"\n",
        "VS_DEPLOYED_INDEX_ID = f\"embeddings_deployed_{VS_ITERATION_SUFFIX}\""
      ],
      "metadata": {
        "id": "JCq3HH_bQlf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init the aiplatform package\n",
        "from google.cloud import aiplatform\n",
        "\n",
        "aiplatform.init(project=PROJECT_ID, location=LOCATION)\n",
        "\n",
        "# create index\n",
        "my_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
        "    display_name=VS_INDEX_NAME,\n",
        "    contents_delta_uri=f\"gs://{EMBEDDINGS_BUCKET}/articles_embeddings.jsonl\",\n",
        "    dimensions=768,\n",
        "    approximate_neighbors_count=20,\n",
        "    distance_measure_type=\"DOT_PRODUCT_DISTANCE\",\n",
        "\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "7tJQx2cY-hT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create IndexEndpoint\n",
        "my_index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
        "    display_name=VS_INDEX_ENDPOINT_NAME, public_endpoint_enabled=True\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "yQlBq8UO_sna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy the Index to the Index Endpoint\n",
        "my_index_endpoint.deploy_index(index=my_index, deployed_index_id=VS_DEPLOYED_INDEX_ID)\n",
        "\n"
      ],
      "metadata": {
        "id": "FAMYQl5sDHUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Running a sample query"
      ],
      "metadata": {
        "id": "CGr2-MeKRVT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_emb = get_embeddings_wrapper( ['diversity and inclusion'])\n",
        "# print(query_emb)\n"
      ],
      "metadata": {
        "id": "_T1q71WwBtbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# run query\n",
        "response = my_index_endpoint.find_neighbors(\n",
        "    deployed_index_id=VS_DEPLOYED_INDEX_ID, queries=query_emb, num_neighbors=10\n",
        ")\n",
        "\n",
        "print(response)\n",
        "\n",
        "# show the results\n",
        "for idx, neighbor in enumerate(response[0]):\n",
        "    print(f\"{neighbor.distance:.2f} {neighbor.id}\")\n"
      ],
      "metadata": {
        "id": "guYjXGkwBG6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RHFH6TWYIYqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_index.to_dict()"
      ],
      "metadata": {
        "id": "qP0dSadfIYxU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEPLOYED_INDEX_ID"
      ],
      "metadata": {
        "id": "jVEjpg8SN_nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_embeddings = get_embeddings_wrapper([\"Growth in cloud\"])"
      ],
      "metadata": {
        "id": "J3nj1l0GGHRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test query\n",
        "response = my_index_endpoint.find_neighbors(\n",
        "    deployed_index_id=VS_DEPLOYED_INDEX_ID,\n",
        "    queries=test_embeddings,\n",
        "    num_neighbors=20,\n",
        ")\n",
        "\n",
        "print(response)\n",
        "\n",
        "# show the result\n",
        "import numpy as np\n",
        "\n",
        "for idx, neighbor in enumerate(response[0]):\n",
        "    id = str(neighbor.id)\n",
        "    similar = df.query(\"id == @id\", engine=\"python\")\n",
        "    print(f\"{neighbor.distance:.4f} {similar.file_name.values[0]}\")"
      ],
      "metadata": {
        "id": "IJuVMzO9GKfL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "[V2.1] Metadata Explorer - Ingestion & Metadata Enrichment.ipynb",
      "collapsed_sections": [
        "xmS3VhBW8ub4",
        "uH327alV8xIv",
        "Le6zvMW6Q4rf",
        "wx9UzEKKUF0I",
        "gJowyYSN2WiF",
        "1Ea-I2b2E3SL",
        "haonbLQ3yUWm",
        "yoOXJbIfFShD",
        "wKbIOjRV2PXb",
        "yQKMjQR5O507",
        "Mt5j0IcjZhVu"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}